[["index.html", "Watershed Analysis: ENSC 445/LRES 545 Chapter 1 Introduction 1.1 Course overview and objectives 1.2 Structure 1.3 Tentative schedule, subject to change", " Watershed Analysis: ENSC 445/LRES 545 Tim Covino 2024-01-22 Chapter 1 Introduction This book provides the materials that we will use in Watershed Analysis (ENSC 445/545). In this class we will be learning the fundamentals of watershed analysis in R. Instructor: Dr. Tim Covino Class times: T 10:50 – 12:05; Th 10:50 – 12:05 Office hours: By appointment Website: https://tpcovino.github.io/ensc_445_545_bookdown 1.1 Course overview and objectives provide theoretical understanding and practical experience with common analysis and modeling techniques relevant to watershed hydrology. provide training in analyzing, simulating, and presenting scientific data in written and oral formats. 1.2 Structure This class will be largely hands-on, and students will be conducting watershed analyses and modeling exercises. We will be doing our analysis and modeling in R and will do some R coding in each class session. Programming is best learned by doing it often. We will generally have “lecture” on Tuesday, where we will talk about and work through various types of hydrological analyses. On Thursday’s you will then put the content from Tuesday to work in a lab where you will complete a variety of hydrological analyses in R. Philosophical approach and books &amp; resources we will utilize This course will use all online and open-source resources and will follow FAIR (findability, accessibility, interoperability, and reusability) data principles and promote sharing of hydrological education and research materials. Our computing will utilze open source R and RStudio software. Books and readings will include R for Data Science and Statistical Methods in Water Resources, but other readings will be made available on this bookdown page as needed. We will promote an open, equitable, and collaborative environment such that we can all succeed and improve our skills in watershed analysis. 1.3 Tentative schedule, subject to change Week 1: - Lecture (1/18): Introduction, review. - Reading: Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). Week 2: - Lecture (1/23): Overview and data viz.  - Work session (1/25): Data visualization, data wrangling, and programming. - Reading: Chapter 2.1: Graphical Analysis of Single Datasets in Statistical Methods in Water Resources (SMWR). AND Chapters 4 &amp; 5 4-Workflow: Basics &amp; 5-Data transformation in RDS. Week 3: - Lab1 (1/30): Data viz, wrangling, and programming. - Lab 1 (2/1): Data viz, wrangling, and programming. Week 4: - Lecture (2/6): Statistics in hydrology. - Lab 2 (2/8): Statistics in hydrology. - Reading: Chapter 1: 1-Summarizing Univariate Data in SMWR Week 5: - Lecture (2/13): Downloading and shaping data frames. - Lab 3 (2/15): Downloading and shaping data frames. - Submit term project ideas - Reading: Introduction to the dataRetrieval package AND Chapter 12 &amp; 13 of R for Data Science Week 6: - Lecture (2/20): Surface water: Rating curves and hydrographs. - Lab 4 (2/22): Rating curves and hydrographs. Week 7: - Lecture (2/27): Surface water: Rating curves and hydrographs. - Lab 4 (2/29): Rating curves and hydrographs - Reading: Chapter 4 4-Hypothesis Tests in SMWR. Week 8: - Lab 5 (3/5): Flow frequency analysis (low flows) - Work session (3/7): Term project work session/guest lecture - Reading: Definitions and characteristics of low flows in EPA Environmental Modeling Community of Practice Week 9: Spring break (3/12 &amp; 3/14) - no class! Week 10: - Lecture (3/19): Cimate trend analysis - Lab 6 (3/21): Trend analysis - Reading: Chapters 12.1 &amp; 12.2 12.1-General Structure of Trend Tests &amp; 12.2-Trend Tests with No Exogenous Variables in SMWR. Week 11: - Work session (3/26): Term project work session - Lab 7 or work session (3/28): Precipitation analysis or work session Week 12: - Lecture/lab (4/2): Geospatial hydrology in R - Lab 8 (4/4): Watershed delineation in R - Reading: Geocomputation with R - Reading: Geospatial analysis in R Week 13: - Lecture/lab (4/9): Term project update presentations - Lab (4/11): Hydrological modeling part 1 - Reading An Overview of Rainfall-Runoff Model Types Week 14: - Lab (4/25): Lab or term project work session - Lab (4/27): Lab or term project work session Week 15: - Lab (4/25): Hydrological modeling part 2 - Lab (4/27): Term project work session Week 16: Term project presentations (5/2 and 5/4) Week 17 (finals week): Submit term project R Markdown knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = TRUE) "],["intro-to-data-visualization.html", "Chapter 2 Intro to data visualization 2.1 Learning Objectives 2.2 Download and install tidyverse library 2.3 Reading data 2.4 Our first ggplot 2.5 Change point type 2.6 Set colors 2.7 Controlling color with a third variable and other functions 2.8 Plotting multiple groups (adding a third variable) 2.9 Facets 2.10 Two variable faceting 2.11 Boxplots 2.12 More about color, size, etc 2.13 Multiple geoms 2.14 Exit ticket", " Chapter 2 Intro to data visualization 2.1 Learning Objectives The learning objectives for today are to begin getting comfortable navigating RStudio. A great way to do this is through data visualization. At the end of todays activities students will be able to: Load data to RStudio Visualize data by making plots/figures Install and load packages Begin coding using tidyverse Have an understanding of projects and folder management Knit .Rmd to .html Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd”. Always be sure to read the README.md files in the GitHub repo. Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. The use of projects is highly recommended and is the practice we will follow in this class. You will learn more about projects later, for now just know they are useful and make things easier. In this unit we want to start familiarizing ourselves with R by visualizing some hydrological data. The reading for this week will also begin to get you more familiar with R and RStudio. Please read Chapters 1, 2, &amp; 3 1-Welcome, 2-Introduction, &amp; 3-Data visualization in R for Data Science (RDS). 2.2 Download and install tidyverse library We will use the tidyverse a lot this semester. It is a suite of packages that handles plotting and data wrangling efficiently. You only have to install the library once. You have to load it using the library() function each time you start an R session. #install.packages(&quot;tidyverse&quot;) library(tidyverse) 2.3 Reading data The following lines will read in the data we will use for this exercise. Don’t worry about this right now beyond running it, we will talk more about it later. Pine &lt;- read_csv(&quot;pine_jan-mar_2010.csv&quot;) SNP &lt;- read_csv(&quot;pine_nfdr_jan-mar_2010.csv&quot;) RBI &lt;- read_csv(&quot;flashy_dat_subset.csv&quot;) Basic ggplot syntax 2.4 Our first ggplot Let’s look at the Pine data, plotting streamflow (the cfs column) by the date (datetime column). We will show the time series as a line. ggplot(data = Pine, aes(x = datetime, y = cfs)) + geom_line() 2.5 Change point type Now let’s make the same plot but show the data as points, using the shape parameter in geom_point() we can change the point type to any of the following: See here ggplot(data = Pine, aes(x = datetime, y = cfs))+ geom_point(shape = 5) 2.6 Set colors We can also “easily” change the color. Easily is in quotes because this often trips people up. If you put color = “blue” in the aesthetic function, think about what that is telling ggplot. It says “control the color using”blue”“. That doesn’t make a whole lot of sense, so neither does the output… Try it. What happens is that if color = “blue” is in the aesthetic, you are telling R that the color used in the geom represents “blue”. This is very useful if you have multiple geoms in your plot, are coloring them differently, and are building a legend. But if you are just trying to color the points, it kind of feels like R is trolling you… doesn’t it? Take the color = “blue” out of the aesthetic and you’re golden. ggplot(data = Pine, aes(x = datetime, y = cfs, color = &quot;blue&quot;))+ geom_point() ggplot(data = Pine, aes(x = datetime, y = cfs))+ geom_point(color = &quot;blue&quot;) 2.7 Controlling color with a third variable and other functions Let’s plot the data as a line again, but play with it a bit. First: make the line blue ggplot(data = Pine, aes(x = datetime, y = cfs)) + geom_line(color = &quot;blue&quot;) Second: change the theme See ggplot themes here ggplot(data = Pine, aes(x = datetime, y = cfs)) + geom_line(color = &quot;blue&quot;) + theme_linedraw() Third: change the axis labels ggplot(data = Pine, aes(x = datetime, y = cfs)) + geom_line(color = &quot;blue&quot;) + theme_linedraw() + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;) Fourth: color by discharge See here for changing axis labels and coloring by a variable (in this case discharge) ggplot(data = Pine, aes(x = datetime, y = cfs, color = cfs)) + geom_line() + theme_linedraw() + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;) 2.8 Plotting multiple groups (adding a third variable) The SNP dataset has two different streams: Pine and NFDR We can look at the two of those a couple of different ways. First, make two lines, colored by the stream by adding color = to your aesthetic. Remember that we can have a look at column headers with head(df), if you need to remind yourself of variable names. ggplot(data = SNP, aes(x = datetime, y = cfs, color = StationID)) + geom_line() Now use what we just did to make that figure look better. ggplot(data = SNP, aes(x = datetime, y = cfs, color = StationID)) + geom_line() + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;, color = &quot;Stream&quot;) + theme_linedraw(base_size = 18) 2.9 Facets We can also use facets. You must tell the facet_wrap what variable to use to make the separate panels (facets =). It’ll decide how to orient them or you can tell it how. We want them to be on top of each other so we are going to tell it we want 2 rows by setting nrow = 2. Note that we have to put the column used to make the facets in quotes after facets = ggplot(data = SNP, aes(x = datetime, y = cfs)) + geom_line() + facet_wrap(facets = &quot;StationID&quot;, nrow = 2) 2.10 Two variable faceting You can also use facet_grid() to break your plots up into panels based on two variables. Below we will create a panel for each month in each watershed. Adding scales = “free” allows facet_grid to change the axes. By default, all axes will be the same. This is often what we want, so we can more easily compare magnitudes, but sometimes we are looking for patterns more, so we may want to let the axes have whatever range works for the individual plots. ggplot(data = SNP, aes(x = datetime, y = cfs)) + geom_line() + facet_grid(StationID ~ month, scales = &quot;free&quot;) 2.11 Boxplots We can look at these data in other ways as well. A very useful way to look at the variation of two groups is to use a boxplot. Because the data span several orders of magnitude, we will have to log the y axis to see the differences between the two streams. We do that by adding scale_y_log10() ggplot(data = SNP, aes(x = StationID, y = cfs)) + stat_boxplot() + scale_y_log10() To investigate the boxplot more closely we can use “plotly”, which generates interactive plots. #install.packages(&quot;plotly&quot;) library(plotly) ggplotly(ggplot(data = SNP, aes(x = StationID, y = cfs)) + stat_boxplot()+ scale_y_log10() ) 2.12 More about color, size, etc Let’s play around a bit with controlling color, point size, etc with other data. We can control the size of points by putting size = in the aes() and color by putting color = ggplot(RBI, aes(x = DRAIN_SQKM, y = RBI, size = T_AVG_SITE, color = STATE))+ geom_point() If you use a point type that has a background, like #21, you can also set the background color using bg = If points are too close together to see them all you can use a hollow point type or set the alpha lower so the points are transparent (alpha = ) ggplot(data = RBI, aes(x = DRAIN_SQKM, y = RBI, size = T_AVG_SITE, bg = STATE))+ geom_point(pch = 21, alpha = 0.3) You can also easily make that an interactive plot with ggplotly. Try doing that. 2.13 Multiple geoms Finally: You can add multiple geoms to the same plot. Examples of when you might want to do this are when you are showing point data and you want to have a line connecting them. Or if you are fitting a linear model to two data sets and you want to show each. These are just two examples. There are many other reasons you would do this. Point being you simply add additional geom_… lines to add additional geoms. See here for information on linetype aesthetics. Using a “dashed” linetype to connect point measurements is a common approach. ggplot(data = RBI, aes(x = RBI, y = DRAIN_SQKM, color = STATE))+ geom_line(linetype = &quot;dashed&quot;) + geom_point() ggplot(data = RBI, aes(x = RBI, y = DRAIN_SQKM, color = AGGECOREGION))+ stat_smooth(method = &quot;lm&quot;, linetype = 2)+ geom_point() That’s it for today! 2.14 Exit ticket Write a code that generates a ggplot from a data frame called df and plots Date on the x-axis, discharge on the y-axis as a line, and colors by gauging_station. knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, results = TRUE) "],["data-wrangling-using-tidyverse.html", "Chapter 3 Data wrangling using tidyverse 3.1 Introduction 3.2 Learning objectives: 3.3 You can use R as a calculator 3.4 You can create new objects using &lt;- 3.5 Using functions 3.6 Read in some data 3.7 What is a tibble? 3.8 Data wrangling in dplyr 3.9 Filter 3.10 Arrange 3.11 Select 3.12 Mutate 3.13 Summarize 3.14 Multiple operations with pipes 3.15 A final comment on NAs 3.16 That’s it for today 3.17 Exit ticket", " Chapter 3 Data wrangling using tidyverse Follow this link to download everything you need for this unit. When you get to GitHub click on “Code” (green button) and select “download zip”. You will then save this to a local folder where you should do all of your work for this class. You will work through the “_blank.Rmd”. Always be sure to read the README.md files in the GitHub repo. Once you have this folder saved where you would like it, open RStudio and navigate to the folder. Next, open the project (“.Rproj”). Doing so will set the folder as the working directory, make your life easier, and make everything generally work. 3.1 Introduction We have messed around with plotting a bit and you’ve seen a little of what R can do. So now let’s review or introduce you to some basics. Even if you have worked in R before, it is good to be remind of/practice with this stuff, so stay tuned in! Reading for this week: Chapter 2.1: Graphical analysis of single datasets in SMWR Workflow: basics Data transformation 3.2 Learning objectives: Working through this exercise will help students: - become more familiar with the RStudio IDE - get in the habit of running single lines of code - know what a tibble is - know what the assignment operator is - begin using base and dplyr functions 3.3 You can use R as a calculator If you just type numbers and operators in, R will spit out the results. It is generally good to run one line of code at a time. In mac you do that by putting your cursor on the line and hitting command + enter. On windows/PC that is ctrl + enter. Here is a link to info on Editing and Executing code in RStudio Very handy link to all keyboard shortcuts Windows, Linux and Mac 1 + 2 ## [1] 3 2 + 2 ## [1] 4 3.4 You can create new objects using &lt;- Yes, = does the same thing. But use &lt;-. We will call &lt;- assignment or assignment operator. When we are coding in R we use &lt;- to assign values to objects and = to set values for parameters in functions/equations/etc. Using &lt;- helps us differentiate between the two. Norms for formatting are important because they help us understand what code is doing, especially when stuff gets complex. Oh, one more thing: Surround operators with spaces. x &lt;- 1 is easier to read than x&lt;-1 You can assign single numbers or entire chunks of data using &lt;- So if you had an object called my_data and wanted to copy it into my_new_data you could do: my_new_data &lt;- my_data You can then recall/print the values in an object by just typing the name by itself. In the code chunk below, assign a 3 to the object “y” and then print it out. # This is a code chunk. # Putting a pound sign in here allows me to type text that is not code. # The stuff below is code. Not text. y &lt;- 3 y ## [1] 3 If you want to assign multiple values, you have to put them in the function c() c means combine. R doesn’t know what to do if you just give it a bunch of values with space or commas, but if you put them as arguments in the combine function, it’ll make them into a vector. Any time you need to use several values, even passing as an argument to a function, you have to put them in c() or it won’t work. a &lt;- c(1,2,3,4) a ## [1] 1 2 3 4 When you are creating objects, try to give them meaningful names so you can remember what they are. You can’t have spaces or operators that mean something else as part of a name. And remember, everything is case sensitive. Assign the value 5.4 to water_pH and then try to recall it by typing “water_ph” water_pH &lt;- 5.4 water_pH ## [1] 5.4 If we want to remove something from the environment we can use rm(). Try to remove water_pH. You can also set objects equal to strings, or values that have letters in them. To do this you just have to put the value in quotes, otherwise R will think it is an object name and tell you it doesn’t exist. Try: name &lt;- “your name” and then name &lt;- your name What happens if you forget the ending parenthesis? R can be cryptic with it’s error messages or other responses, but once you get used to them, you know exactly what is wrong when they pop up. As a note - when you go to the internet for example code it will often say things like df &lt;- your_data, this is similar to what I’ve written above: name &lt;- “your name”. It means enter you name (or your data). As you progress you will get better at understanding example code and understanding error messages. name &lt;- &quot;Tim&quot; #name &lt;- Tim 3.5 Using functions As an example, let’s try the seq() function, which creates a sequence of numbers. seq(from = 1, to = 10, by = 1) # these are base R functions ## [1] 1 2 3 4 5 6 7 8 9 10 # or seq(1, 10, 1) ## [1] 1 2 3 4 5 6 7 8 9 10 # or seq(1, 10) ## [1] 1 2 3 4 5 6 7 8 9 10 # Predict what this does seq(10,1) ## [1] 10 9 8 7 6 5 4 3 2 1 3.6 Read in some data First we will load the tidyverse library, everything we have done so far today is in base R. Next, let’s load a few dataframes and have a look at them. We will load the “PINE_NFDR_Jan-Mar_2010.csv” and “flashy_dat_all.csv” files. Important: read_csv() is the tidyverse csv reading function, the base R function is read.csv(). read.csv() will not read your data in as a tibble, which is the format used by tidyverse functions. You should get in the habit of using the tidyverse versions such as read_csv(). library(tidyverse) flow &lt;- read_csv(&quot;pine_nfdr_jan-mar_2010.csv&quot;) rbi &lt;- read_csv(&quot;flashy_dat_all.csv&quot;) 3.7 What is a tibble? Good question. It’s a fancy way to store data that works well with tidyverse functions. Let’s look at the flow tibble with “head” and “str” head(flow) ## # A tibble: 6 × 8 ## StationID cfs surrogate datetime year quarter month ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PINE 11.6 N 2010-01-01 00:00:00 2010 1 1 ## 2 PINE 11.6 N 2010-01-01 01:00:00 2010 1 1 ## 3 PINE 11.2 N 2010-01-01 02:00:00 2010 1 1 ## 4 PINE 11.2 N 2010-01-01 03:00:00 2010 1 1 ## 5 PINE 11.2 N 2010-01-01 04:00:00 2010 1 1 ## 6 PINE 11.2 N 2010-01-01 05:00:00 2010 1 1 ## # ℹ 1 more variable: day &lt;dbl&gt; str(flow) ## spc_tbl_ [4,320 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ StationID: chr [1:4320] &quot;PINE&quot; &quot;PINE&quot; &quot;PINE&quot; &quot;PINE&quot; ... ## $ cfs : num [1:4320] 11.6 11.6 11.2 11.2 11.2 ... ## $ surrogate: chr [1:4320] &quot;N&quot; &quot;N&quot; &quot;N&quot; &quot;N&quot; ... ## $ datetime : POSIXct[1:4320], format: &quot;2010-01-01 00:00:00&quot; ... ## $ year : num [1:4320] 2010 2010 2010 2010 2010 2010 2010 2010 2010 2010 ... ## $ quarter : num [1:4320] 1 1 1 1 1 1 1 1 1 1 ... ## $ month : num [1:4320] 1 1 1 1 1 1 1 1 1 1 ... ## $ day : num [1:4320] 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. StationID = col_character(), ## .. cfs = col_double(), ## .. surrogate = col_character(), ## .. datetime = col_datetime(format = &quot;&quot;), ## .. year = col_double(), ## .. quarter = col_double(), ## .. month = col_double(), ## .. day = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Now read in the same data with read.csv() which will NOT read the data as a tibble. How is it different? Output each one in the Console. Knowing the data type for each column is super helpful for a few reasons…. let’s talk about them. flow_NT &lt;- read.csv(&quot;pine_nfdr_jan-mar_2010.csv&quot;) # this is base R head(flow_NT) ## StationID cfs surrogate datetime year quarter ## 1 PINE 11.58 N 2010-01-01T00:00:00Z 2010 1 ## 2 PINE 11.58 N 2010-01-01T01:00:00Z 2010 1 ## 3 PINE 11.24 N 2010-01-01T02:00:00Z 2010 1 ## 4 PINE 11.24 N 2010-01-01T03:00:00Z 2010 1 ## 5 PINE 11.24 N 2010-01-01T04:00:00Z 2010 1 ## 6 PINE 11.24 N 2010-01-01T05:00:00Z 2010 1 ## month day ## 1 1 1 ## 2 1 1 ## 3 1 1 ## 4 1 1 ## 5 1 1 ## 6 1 1 head(flow) ## # A tibble: 6 × 8 ## StationID cfs surrogate datetime year quarter month ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PINE 11.6 N 2010-01-01 00:00:00 2010 1 1 ## 2 PINE 11.6 N 2010-01-01 01:00:00 2010 1 1 ## 3 PINE 11.2 N 2010-01-01 02:00:00 2010 1 1 ## 4 PINE 11.2 N 2010-01-01 03:00:00 2010 1 1 ## 5 PINE 11.2 N 2010-01-01 04:00:00 2010 1 1 ## 6 PINE 11.2 N 2010-01-01 05:00:00 2010 1 1 ## # ℹ 1 more variable: day &lt;dbl&gt; # We can remove the flow_NT dataframe from the enviroment with rm(flow_NT) rm(flow_NT) 3.8 Data wrangling in dplyr If you forget syntax or what the following functions do, here is a cheat sheet: https://rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf We will demo five functions below (these are tidyverse/dplyr functions): filter() - returns rows that meet specified conditions arrange() - reorders rows select() - pull out variables (columns) mutate() - create new variables (columns) or reformat existing ones summarize() - collapse groups of values into summary stats 3.9 Filter Write an expression that returns data in rbi for the state of Montana (MT) filter(rbi, STATE == &quot;MT&quot;) ## # A tibble: 17 × 26 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6037500 0.0338 8 Madison River… 1126. 10U 44.7 ## 2 6043500 0.0724 61 Gallatin Rive… 2120. 10U 45.5 ## 3 6073500 0.104 123 Dearborn Rive… 835. 10U 47.2 ## 4 6154410 0.118 157 Little People… 33.4 10U 48.0 ## 5 6187915 0.110 143 Soda Butte Cr… 73.1 10U 45.0 ## 6 6191000 0.0627 46 Gardner River… 514. 10U 45.0 ## 7 6191500 0.0649 49 Yellowstone R… 6784. 10U 45.1 ## 8 6289000 0.0652 50 Little Bighor… 471. 10U 45.0 ## 9 6291500 0.0923 94 Lodge Grass C… 218 10U 45.1 ## 10 12358500 0.0922 93 Middle Fork F… 2939. 17 48.5 ## 11 12374250 0.0848 77 Mill Cr ab Ba… 50.8 17 47.8 ## 12 12375900 0.0985 113 South Crow Cr… 19.7 17 47.5 ## 13 12377150 0.118 153 Mission Cr ab… 32.2 17 47.3 ## 14 12381400 0.0698 55 South Fork Jo… 151. 17 47.2 ## 15 12383500 0.0470 19 Big Knife Cre… 17.7 17 47.1 ## 16 12388400 0.100 117 Revais Cr bl … 60.8 17 47.3 ## 17 12390700 0.0738 64 Prospect Cree… 470. 17 47.6 ## # ℹ 19 more variables: LNG_GAGE &lt;dbl&gt;, STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, ## # FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt; And one that keeps flows less than 100 cfs in the “flow” dataframe. filter(flow, cfs &lt; 100) ## # A tibble: 4,169 × 8 ## StationID cfs surrogate datetime year quarter ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 PINE 11.6 N 2010-01-01 00:00:00 2010 1 ## 2 PINE 11.6 N 2010-01-01 01:00:00 2010 1 ## 3 PINE 11.2 N 2010-01-01 02:00:00 2010 1 ## 4 PINE 11.2 N 2010-01-01 03:00:00 2010 1 ## 5 PINE 11.2 N 2010-01-01 04:00:00 2010 1 ## 6 PINE 11.2 N 2010-01-01 05:00:00 2010 1 ## 7 PINE 11.2 N 2010-01-01 06:00:00 2010 1 ## 8 PINE 11.2 N 2010-01-01 07:00:00 2010 1 ## 9 PINE 10.9 N 2010-01-01 08:00:00 2010 1 ## 10 PINE 10.9 N 2010-01-01 09:00:00 2010 1 ## # ℹ 4,159 more rows ## # ℹ 2 more variables: month &lt;dbl&gt;, day &lt;dbl&gt; Above we just executed the operation, but didn’t save it. Let’s save that work using the assignment operator. rbi_mt &lt;- filter(rbi, STATE == &quot;MT&quot;) low_flows &lt;- filter(flow, cfs &lt; 100) 3.9.1 Multiple conditions How many gages are there in Montana with an rbi greater than 0.05 filter(rbi, STATE == &quot;MT&quot; &amp; RBI &gt; 0.05) ## # A tibble: 15 × 26 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6043500 0.0724 61 Gallatin Rive… 2120. 10U 45.5 ## 2 6073500 0.104 123 Dearborn Rive… 835. 10U 47.2 ## 3 6154410 0.118 157 Little People… 33.4 10U 48.0 ## 4 6187915 0.110 143 Soda Butte Cr… 73.1 10U 45.0 ## 5 6191000 0.0627 46 Gardner River… 514. 10U 45.0 ## 6 6191500 0.0649 49 Yellowstone R… 6784. 10U 45.1 ## 7 6289000 0.0652 50 Little Bighor… 471. 10U 45.0 ## 8 6291500 0.0923 94 Lodge Grass C… 218 10U 45.1 ## 9 12358500 0.0922 93 Middle Fork F… 2939. 17 48.5 ## 10 12374250 0.0848 77 Mill Cr ab Ba… 50.8 17 47.8 ## 11 12375900 0.0985 113 South Crow Cr… 19.7 17 47.5 ## 12 12377150 0.118 153 Mission Cr ab… 32.2 17 47.3 ## 13 12381400 0.0698 55 South Fork Jo… 151. 17 47.2 ## 14 12388400 0.100 117 Revais Cr bl … 60.8 17 47.3 ## 15 12390700 0.0738 64 Prospect Cree… 470. 17 47.6 ## # ℹ 19 more variables: LNG_GAGE &lt;dbl&gt;, STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, ## # FLOWYRS_1990_2009 &lt;dbl&gt;, wy00_09 &lt;dbl&gt; Challenge: Filter for flow less than 100 cfs just for the NFDR gauge in “flow”. 3.10 Arrange Arrange sorts by a column in your dataset. Sort the rbi data by the RBI column in ascending and then descending order arrange(rbi, RBI) ## # A tibble: 1,144 × 26 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 6408700 0.0108 1 RHOADS FORK N… 20.8 10U 44.1 ## 2 6430850 0.0192 2 LITTLE SPEARF… 71.8 10U 44.3 ## 3 10244950 0.0230 3 STEPTOE C NR … 28.2 16 39.2 ## 4 2267000 0.0286 4 CATFISH CREEK… 169. 3 28.0 ## 5 6775500 0.0300 5 MIDDLE LOUP R… 5460. 10L 41.8 ## 6 6430532 0.0303 6 CROW CREEK NE… 106. 10U 44.6 ## 7 14158500 0.0306 7 MCKENZIE RIVE… 237. 17 44.4 ## 8 6037500 0.0338 8 Madison River… 1126. 10U 44.7 ## 9 10109001 0.0382 9 COM F LOGAN R… 556. 16 41.7 ## 10 10243700 0.0391 10 CLEVE C NR EL… 83.5 16 39.2 ## # ℹ 1,134 more rows ## # ℹ 19 more variables: LNG_GAGE &lt;dbl&gt;, STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, … arrange(rbi, desc(RBI)) ## # A tibble: 1,144 × 26 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE LNG_GAGE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9.49e6 1.76 1144 CANADA… 675. 15 32.3 -111. ## 2 9.51e6 1.75 1143 SKUNK … 170. 15 33.7 -112. ## 3 8.40e6 1.62 1142 SOUTH … 534 13 32.6 -104. ## 4 9.49e6 1.58 1141 BRAWLE… 2028. 15 32.1 -111. ## 5 9.54e6 1.57 1140 SAN SI… 1483. 15 32.0 -112. ## 6 8.20e6 1.56 1139 Seco C… 435. 12 29.4 -99.3 ## 7 1.11e7 1.55 1138 Lytle … 365. 18 34.1 -117. ## 8 7.02e6 1.51 1137 Fishpo… 24.9 7 38.6 -90.5 ## 9 7.23e6 1.50 1136 Palo D… 2909. 11 36.2 -101. ## 10 6.85e6 1.40 1135 BEAVER… 4358. 10L 40.0 -101. ## # ℹ 1,134 more rows ## # ℹ 18 more variables: STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, … 3.11 Select Look at the RBI dataframe. There are too many columns! You will often want to get rid of some columns and clean up the dataframe (df) for analysis. Select Site name, state, and RBI from the rbi data Note they come back in the order you put them in in the function, not the order they were in in the original data. You can do a lot more with select, especially when you need to select a bunch of columns but don’t want to type them all out. For example, if you want to select a group of columns you can specify the first and last with a colon in between (first:last) and it’ll return all of them. Select the rbi columns from site_no to DRAIN_SQKM. You can also remove one column with select(-column). Remove the “surrogate” column from flow. rbi_mt_thin &lt;- select(rbi, STANAME, STATE, RBI) rbi_thin &lt;- select(rbi, site_no:DRAIN_SQKM) flow_thin &lt;- select(flow, -surrogate) 3.12 Mutate Use mutate to add new columns based on additional ones. Common uses are to create a column of data in different units, or to calculate something based on two columns. You can also use it to just update a column, by naming the new column the same as the original one (but be careful because you’ll lose the original one!). Create a new column in rbi called T_RANGE by subtracting T_MIN_SITE from T_MAX_SITE mutate(rbi, T_RANGE = T_MAX_SITE - T_MIN_SITE) ## # A tibble: 1,144 × 27 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1013500 0.0584 35 Fish River nea… 2253. 1 47.2 ## 2 1021480 0.208 300 Old Stream nea… 76.7 1 44.9 ## 3 1022500 0.198 286 Narraguagus Ri… 574. 1 44.6 ## 4 1029200 0.132 183 Seboeis River … 445. 1 46.1 ## 5 1030500 0.114 147 Mattawamkeag R… 3676. 1 45.5 ## 6 1031300 0.297 489 Piscataquis Ri… 304. 1 45.3 ## 7 1031500 0.320 545 Piscataquis Ri… 769 1 45.2 ## 8 1037380 0.318 537 Ducktrap River… 39 1 44.3 ## 9 1044550 0.242 360 Spencer Stream… 500. 1 45.3 ## 10 1047000 0.344 608 Carrabassett R… 909. 1 44.9 ## # ℹ 1,134 more rows ## # ℹ 20 more variables: LNG_GAGE &lt;dbl&gt;, STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, … When downloading data from the USGS through R, you have to enter the gage ID as a character, even though they are all made up of numbers. So to practice doing this, update the site_no column to be a character datatype mutate(rbi, site_no = as.character(site_no)) ## # A tibble: 1,144 × 26 ## site_no RBI RBIrank STANAME DRAIN_SQKM HUC02 LAT_GAGE ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1013500 0.0584 35 Fish River nea… 2253. 1 47.2 ## 2 1021480 0.208 300 Old Stream nea… 76.7 1 44.9 ## 3 1022500 0.198 286 Narraguagus Ri… 574. 1 44.6 ## 4 1029200 0.132 183 Seboeis River … 445. 1 46.1 ## 5 1030500 0.114 147 Mattawamkeag R… 3676. 1 45.5 ## 6 1031300 0.297 489 Piscataquis Ri… 304. 1 45.3 ## 7 1031500 0.320 545 Piscataquis Ri… 769 1 45.2 ## 8 1037380 0.318 537 Ducktrap River… 39 1 44.3 ## 9 1044550 0.242 360 Spencer Stream… 500. 1 45.3 ## 10 1047000 0.344 608 Carrabassett R… 909. 1 44.9 ## # ℹ 1,134 more rows ## # ℹ 19 more variables: LNG_GAGE &lt;dbl&gt;, STATE &lt;chr&gt;, CLASS &lt;chr&gt;, ## # AGGECOREGION &lt;chr&gt;, PPTAVG_BASIN &lt;dbl&gt;, PPTAVG_SITE &lt;dbl&gt;, ## # T_AVG_BASIN &lt;dbl&gt;, T_AVG_SITE &lt;dbl&gt;, T_MAX_BASIN &lt;dbl&gt;, ## # T_MAXSTD_BASIN &lt;dbl&gt;, T_MAX_SITE &lt;dbl&gt;, T_MIN_BASIN &lt;dbl&gt;, ## # T_MINSTD_BASIN &lt;dbl&gt;, T_MIN_SITE &lt;dbl&gt;, PET &lt;dbl&gt;, ## # SNOW_PCT_PRECIP &lt;dbl&gt;, PRECIP_SEAS_IND &lt;dbl&gt;, … 3.13 Summarize Summarize will perform an operation on all of your data, or groups if you assign groups. Use summarize to compute the mean, min, and max rbi rbi_sum &lt;- summarize(rbi, meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI)) Now use the group function to group rbi by state and then summarize in the same way as above, but for the full r rbi_state &lt;- group_by(rbi, STATE) rbi_state &lt;- summarize(rbi_state, meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI)) 3.14 Multiple operations with pipes You will note that your environment is filling up with objects. We can eliminate many of those by using pipes. The pipe operator %&gt;% allows you to perform multiple operations in a sequence without saving intermediate steps. Not only is this more efficient, but structuring operations with pipes is also more intuitive than nesting functions within functions (the other way you can do multiple operations). When you use the pipe, it basically takes whatever came out of the first function and puts it into the data argument for the next one, so: rbi %&gt;% group_by(STATE) is the same as group_by(rbi, STATE) Take the groupby and summarize code from above and perform the operation using the pipe rbi_sum &lt;- rbi %&gt;% group_by(STATE) %&gt;% summarize(meanrbi = mean(RBI), maxrbi = max(RBI), minrbi = min(RBI)) 3.15 A final comment on NAs We will talk more about this when we discuss stats, but some operations will fail if there are NA’s in the data. If appropriate, you can tell functions like mean() to ignore NAs by using na.rm = TRUE. You can also use drop_na() if you’re working with a tibble. But be aware if you use that and save the result, drop_na() gets rid of the whole row, not just the NA. Because what would you replace it with…. an NA? First, lets create a small data frame called x that includes: 1, 2, 3, 4, NA. How do we do that? x &lt;- c(1,2,3,4,NA) Next, lets take the mean of x. mean(x) ## [1] NA How do you think we can fix this problem? mean(x, na.rm = TRUE) ## [1] 2.5 3.16 That’s it for today Can you run all of the code in this unit? Do you get errors? If you are working on a lab computer have you gotten your folder management set up? 3.17 Exit ticket If you have an error or problem we need to manage - let me know. If not write NA! "],["lab-1-data-vis-wrangling-and-programming-45-pts.html", "Chapter 4 Lab 1: Data vis, wrangling, and programming (45 pts) 4.1 Problem 1 (5 pts) 4.2 Problem 2 (10 pts) 4.3 Problem 3 (10 pts) 4.4 Problem 4 (10 pts) 4.5 Problem 5 (5 pts) 4.6 Problem 6 (5 pts) 4.7 Summary", " Chapter 4 Lab 1: Data vis, wrangling, and programming (45 pts) You can find the repo for this lab here As we have done previously, go to Code, download zip and put folder on your computer. Open the project in that folder and then open the .Rmd. Remember that opening the project from that folder will set that folder as the “working directory”. Doing so means that when you read in your data, R will “look” in the right place to find it. Remember to rename the .Rmd file with your name in the file name. This lab covers workflow and functions that you have seen and a few things that haven’t been explicitly demonstrated. Using functions that haven’t been explicitly demonstrated will help you in developing your ability to use new functions and workflows. Understanding how to utilize resources like Stack Overflow and package vignettes will help you solve coding problems. You can also get function and package help from the console by typing “?”. For example, you could type “?ggplot” or “?tidyverse” into the console and a help window will appear in the lower right. Also, when you are searching for coding help it is useful to be explicit about whether you are in base R or tidyverse. 4.1 Problem 1 (5 pts) Load the tidyverse library. Read in the pine_nfdr csv using read_csv(). Name this “pine_nfdr”. Make a plot with the date on the x axis, discharge on the y axis. Show the discharge of the two watersheds as a line, coloring by watershed (StationID). Use a theme to remove the grey background. Label the axes appropriately with labels and units. Bonus: change the title of the legend to “Gauging Station” 4.2 Problem 2 (10 pts) Make a boxplot to compare the discharge of Pine to NFDR for February 2010. Hint: use the pipe operator and the filter() function. 4.3 Problem 3 (10 pts) Read in the flashy csv. Name this “flashy”. Create a new df called flashy_west that includes data for: MT, ID, WY, UT, CO, NV, AZ, and NM For only sites in flashy_west: Plot PET (Potential Evapotranspiration) on the X axis and RBI (flashiness index) on the Y axis. Color the points based on what state they are in. Use the linedraw ggplot theme. 4.4 Problem 4 (10 pts) We want to look at the amount of snow for each site in the flashy_west df. Problem is, we are only given the average amount of total precip (PPTAVG_BASIN) and the percentage of snow (SNOW_PCT_PRECIP). Create a new column in the df called “snow_avg_basin” and make it equal to the average total precip times the percentage of snow (careful with the percentage number). Make a barplot showing the amount of snow for each site in MT. Put station name on the x axis and snow amount on the y. You have to add something to geom_bar() to use it for a 2 variable plot. Use “?geom_bar” in the console and the internet to investigate. The x axis of the resulting plot looks terrible! Rotate the X axis labels so we can read them. 4.5 Problem 5 (5 pts) Create a new tibble called “flashy_west_thin” that contains the min, max, and mean PET for each state in flashy_west. Sort/arrange the tibble by mean PET from high to low. Give your columns meaningful names within the summarize function or using rename(). You haven’t seen rename yet. Use ?rename in the console or search for examples. When searching you need to indicate that you are looking for “tidyverse rename”. Rename is part of dplyr, which is part of tidyverse. Be sure your code outputs the tibble. 4.6 Problem 6 (5 pts) Take the tibble from problem 5 and create a new df by first creating a new column that is the Range of the PET (max PET - min PET). Then get rid of the max PET and min PET columns so the tibble just has columns for State, mean_PET, and PET_range. Be sure your code outputs the tibble. Save new_tib_thin as .csv files to your folder for this lab using the write_csv function. To get help on this function type “?write_csv” into the console. 4.7 Summary How long did this lab take you? Is there anything in particular you would like more practice with? Knit this .Rmd as an .html and submit your lab_1_your_name.html on D2L. "],["statistics-in-hydrology---unit-3.html", "Chapter 5 Statistics in hydrology - Unit 3 5.1 Learning objectives 5.2 Homework Q1 (2 pts) 5.3 What is the difference between a sample and a population? 5.4 Homework Q2 (2 pts) 5.5 Measuring our sample distribution: central tendency 5.6 So what’s a weighted average? 5.7 Measures of variability 5.8 Homework Q3 (2 pts) 5.9 What is a normal distribution and how can we determine if we have one? 5.10 Homework Q4 (2 pts) 5.11 Homework Q5 (2 pts) 5.12 Deliverable (10 pts, due Thursday 2/2 by 11:59 PM)", " Chapter 5 Statistics in hydrology - Unit 3 You can find the repo for this activity here 5.1 Learning objectives Describe the characteristics of normal and non-normal distributions Understand the difference between a sample and a population Quantitatively describe data distributions Test data sets for normality Reading for this section Statistical Methods in Water Resources: Chapter 1 Today will use a new package called patchwork that is an extension to ggplot. See here for ggplot extensions including patchwork. library(tidyverse) library(patchwork) First let’s generate some synthetic data and talk about how to visualize it. # generate a normal distribution ExNorm &lt;- rnorm(1000, mean = 5) %&gt;% as_tibble() # look at distributions # first using a histogram ExNorm %&gt;% ggplot(aes(value)) + geom_histogram() # next using the pdf ExNorm %&gt;% ggplot(aes(value)) + stat_density() #Let&#39;s generate a plot that makes comparing these two easier Stack plots to compare histogram and pdf We will save each plot as “ggplot object” and then output them using the patchwork package (loaded in the setup chunk). We have made other objects, like data frames or values, how do you think you would make a ggplot object? # histogram exhist &lt;- ExNorm %&gt;% ggplot(aes(value)) + geom_histogram() # pdf expdf &lt;- ExNorm %&gt;% ggplot(aes(value)) + stat_density() # put the plots side by side with + or on top of each other with / # I&#39;ve also shown how to put a caption into a patchwork figure that has multiple panels. p &lt;- exhist/expdf p + labs(caption = &quot;Figure 1. Comparison of histogram (top) and probability density function (bottom) of a synthetic data set generated with\\n the rnorm function, 1000 samples, and a mean of 5.&quot;) + theme(plot.caption = element_text(hjust = 0)) 5.2 Homework Q1 (2 pts) What is the difference between a histogram and a pdf? What features of the histogram are preserved? Which are lost? 5.3 What is the difference between a sample and a population? Simply put: a population is the thing you are trying to measure. A sample is the data you measure in an effort to measure the population. A sample is a subset of a population. Let’s write some code for an example: We will create a POPULATION that is a large set of numbers. Think of this is as the concentration of Calcium in every bit of water in a lake. Then we will create a SAMPLE by randomly grabbing values from the POPULATION. This simulates us going around in a boat and taking grab samples in an effort to figure out the concentration of calcium in the lake. We can then run this code a bunch of times, you’ll get a different sample each time. You can also take a smaller or larger number of samples by changing “size” in the sample() function. all_the_water &lt;- rnorm(10000, mean = 6) %&gt;% as_tibble() sample_of_water &lt;- sample(all_the_water$value, size = 100, replace = FALSE) %&gt;% as_tibble() population_hist &lt;- all_the_water %&gt;% ggplot(aes(value))+ geom_histogram()+ ggtitle(&quot;Population: \\n All water in lake&quot;) sample_hist &lt;- sample_of_water %&gt;% ggplot(aes(value))+ geom_histogram()+ ggtitle(&quot;Your sample \\n of the lake&quot;) population_hist + sample_hist # OR sample_hist/population_hist 5.4 Homework Q2 (2 pts) How does your sample distribution look similar or different from the population? Why does the sample change every time you run it? What happens as you increase or decrease the number of samples? What happens if you set the number of samples to the size of the population? 5.5 Measuring our sample distribution: central tendency When we take a sample of a population, there are a few things we will want to measure about the distribution of values: where is the middle, how variable is it, and is it skewed to one side or another? The first of these, “where is the middle?” is addressed with measures of central tendency. We will discuss three possible ways to measure this. The mean, median, and weighted mean. To explain the importance of choosing between the mean and median, we will first import some discharge data. Read in the PINE discharge data. pineQ &lt;- read_csv(&quot;PINE_Jan-Mar_2010.csv&quot;) To find the mean (average), you just sum up all the values in your sample and divide by the number of values. To find the median, you put the values IN ORDER, and choose the middle value. The middle value is the one where there are the same number of values higher than that value as there are values lower than it. Because it uses the order of the values rather than just the values themselves, the median is resistant to skewed distributions. This means it is less effected by very large or very small values compared to most values in the sample data. Let’s look at our normal distribution from earlier (ExNorm) compared to the Pine watershed discharge (pineQ) Note that distributions like pineQ, that are positively skewed, are very common in environmental data. # Calculate mean and median for &quot;cfs&quot; in pineQ and &quot;values&quot; in ExNorm pineMean &lt;- mean(pineQ$cfs) pineMedian &lt;- median(pineQ$cfs) xmean &lt;- mean(ExNorm$value) xmedian &lt;- median(ExNorm$value) # plot mean and median on the ExNorm distribution Ex &lt;- ExNorm %&gt;% ggplot(aes(value)) + geom_histogram()+ geom_vline(xintercept = xmean, color = &quot;red&quot;) + geom_vline(xintercept = xmedian, color = &quot;blue&quot;) #plot mean and median on the pineQ discharge histogram PineP &lt;- pineQ %&gt;% ggplot(aes(cfs)) + geom_histogram()+ geom_vline(xintercept = pineMean, color = &quot;red&quot;)+ geom_vline(xintercept = pineMedian, color = &quot;blue&quot;) Ex / PineP 5.6 So what’s a weighted average? When you compute a standard mean or median, you are giving equal weight to each measurement. Adding up all the values in a sample and dividing by the number of samples is the same as multiplying each value by 1/# of samples. For instance if you had ten samples, to calculate the mean you would add them up and divide by 10. This is the same as multiplying each value by 1/10 and then adding them up. Each value is equally weighted at 1/10. There are certain situations in which this is not the ideal way to calculate an average. A common one in hydrology is that you have samples that are supposed to represent different portions of an area. One sample may be taken to measure a forest type that takes up 100 ha of a watershed while another sample represents a forest type that only takes up 4 ha. You may not want to simply average those values! Another example is precipitation gages. In the image below, you see there are 5 rain gages. To get a precipitation number for the watershed, we could just average them, or we could assume they represent an area of the watershed and then weight their values by the area they represent. One method of designating the areas is by using Theissen polygons (the middle watershed). Another method of weighting is isohyetal contours, but we won’t worry about that for now! In the weighted situation, we find the average by multiplying each precipitation values by the proportion of the watershed it represents, shown by the Thiessen polygons, and then add them all together. Let’s do an example. source: https://edx.hydrolearn.org/assets/courseware/v1/e5dc65098f1e8c5faacae0e171e28ccf/asset-v1:HydroLearn+HydroLearn401+2019_S2+type@asset+block/l2_image004.png The precip values for the watershed above are 4.5, 5.5, 5.8, 4.7, and 3.0 We will assume the proportions of the watershed that each gauge represents are 0.20, 0.15, 0.40, 0.15, 0.10, respectively (or 20%, 15%, 40%, 15%, 10%) Challenge: Write some code to compute the regular mean precip from the values, and then the weighted mean. 5.7 Measures of variability Measures of variability allow us to measure the width of our sample data histogram or pdf. If all the values in our sample are close together, we would have small measures of variability, and a pointy pdf/histogram. If they vary more, we would have larger measures of variability and a broad pdf/histogram. We will explore four measures of variability: 5.7.0.1 Variance: Sum of the squared difference of each value from the mean divided by the number of samples minus 1. In R code “var()” (https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf) source: https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf 5.7.0.2 Standard deviation: The square root of the variance. In R code “sd()” **Both variance and standard deviation are sensitive to outliers. 5.7.0.3 CV: Coefficient of Variation CV is simply the standard deviation divided by the mean of the data. Because you divide by the mean, CV is dimensionless. This allows you to use it to compare the variation across distributions with very different magnitudes (e.g., discharge at different gauges). 5.7.0.4 IQR: Interquartile Range IQR is resistant to outliers because it works like a median. It measures the range of the middle 50% of the data in your distribution. So the IQR is the difference between the 75th and 25th percentiles of your data, where the 75th percentile means 75% of the data is BELOW that value and the 25th percentile means 25% is below that value. Using the same vocabulary, the median is the same as the 50th percentile of the data. If you ask R for the QUANTILES of your sample data, it will give you the values at which 0%, 25%, 50%, 75%, and 100% of the data are below. These are the 1,2,3,4, and 5th quantiles. Therefore, the IQR is the difference between the 4th and 2nd quantile. Okay, code time. First, let’s explore how changing the variability of a distribution changes the shape of it’s distribution. Create a plot a random normal distribution using rnorm() and set sd to different numbers. Make the mean of the distribution 0, the sample size 300, and the standard deviation 1 to start. Then increase the standard deviation incrementally to 10 and see what happens. Make the limits of the x axis on the plot -30 to 30. rnorm(300, mean = 0, sd = 1) %&gt;% as_tibble %&gt;% ggplot(aes(value))+ stat_density()+ xlim(c(-30,30)) 5.8 Homework Q3 (2 pts) What happens to the shape of the distribution as the SD increases? If you were to plot the pdf of two hydrographs, one that is flashy, one that isn’t - predict what those distributions would look like. Now let’s calculate the standard deviation, variance, coefficient of variation, and IQR of the Pine discharge data. # standard deviation sd(pineQ$cfs) ## [1] 84.47625 # variance var(pineQ$cfs) ## [1] 7136.237 # coefficient of variation sd(pineQ$cfs)/mean(pineQ$cfs) ## [1] 2.800221 # IQR using the IQR funciton IQR(pineQ$cfs) ## [1] 8.1325 # IQR using the quantile function quants &lt;- quantile(pineQ$cfs) quants[4] - quants[2] ## 75% ## 8.1325 5.8.0.1 What about how lopsided the distribution is? There are several ways to measure this as well, but we are just going to look at one: The Quartile skew. The quartile skew is the difference between the upper quartiles (50th-75th) and the lower quartiles (25th-50th) divided by the IQR (75th-25th). source: https://pubs.usgs.gov/tm/04/a03/tm4a3.pdf Let’s look at the quartile skew of the two distributions we’ve been measuring. Calculate it for the pineQ discharge data and the random normal distribution we generated. Which one is more skewed? quantsP &lt;- quantile(pineQ$cfs) ((quantsP[3]-quantsP[2]) - (quantsP[2] - quantsP[1])) / quantsP[3] - quantsP[1] ## 50% ## -4.837233 quantsX &lt;- quantile(ExNorm$value) ((quantsX[3]-quantsX[2]) - (quantsX[2] - quantsX[1])) / quantsX[3] - quantsX[1] ## 50% ## -2.194869 5.9 What is a normal distribution and how can we determine if we have one? The distribution we generated with rnorm() is a normal distribution. The distribution of pineQ discharge is not normal. Now that we’ve looked at different ways to characterize distributions, we have the vocabulary to describe why. Normal distributions: mean = median, half values to the right, half to the left symmetric (not skewed) single peak Many statistical tests require that the distribution of the data you put into them is normally distributed. BE CAREFUL! There are also tests that use ranked data. Similar to how the median is resistant to outliers, these rank-based tests are resistant to non-normal data. Two popular ones are Kruskal-Wallis and Wilcoxon rank-sum. But how far off can you be before you don’t consider a distribution normal? Seems like a judgement call! R to the rescue! There is a built in test for normality called shapiro.test(), which performs the Shapiro-Wilk test of normality. The hypothesis this test tests is “The distribution is normal.” So if this function returns a p-value less than 0.05, you reject that hypothesis and your distribution is NOT normal. You can also make a quantile-quantile plot. A straight line on this plot indicates a normal distribution, a non-straight line indicates it is not normal. Let’s test Pine discharge data for normality with the Shapiro test and the Q-Q test. shapiro.test(pineQ$cfs) ## ## Shapiro-Wilk normality test ## ## data: pineQ$cfs ## W = 0.27155, p-value &lt; 2.2e-16 qqnorm(pineQ$cfs) Let’s also look at the Q-Q plot with some synthetic data.  Create a normal data set and play with the number of values (n, samples). Investigate how changing the number of samples influences the linearity of the Q-Q plot. Also run this code multiple times with the same n and see if/how the results change. norm &lt;- rnorm(n = 10, mean = 0, sd = 1) %&gt;% as_tibble() qqnorm(norm$value) 5.10 Homework Q4 (2 pts) Do the results change more from run to run when the n is high or low and why? 5.11 Homework Q5 (2 pts) Test your all_the_water, ExNorm, and sample_of_water data frames for normality using shapiro test and qqnorm. Note: The shapiro.test will only accept 5000 values so the all_the_water data frame is too large. The reason for this is that in many fields (e.g., social science) an n of 5000 would be VERY large and would result in a normal distribution. In hydrology and the earth and environmental sciences in genreal we routinely have data sets that are MUCH larger than n = 5000 and the are STILL non-normal. To manage this situation we can use a different test, the Kolmogorov-Smirnov normality test which is in the “nortest” package and the test is called lillie.test. Lillie is short for Lilliefors test which is based on the Koolmogorv-Smirnov test. See below See here for documentation of tests contained in the nortest package # shapiro.test(all_the_water$value) # won&#39;t work. n is too big. # but we can use a test that CAN take large sample sizes. Here we use the Lilliefors test. #install.packages(&quot;nortest&quot;) library(nortest) lillie.test(all_the_water$value) ## ## Lilliefors (Kolmogorov-Smirnov) normality test ## ## data: all_the_water$value ## D = 0.0057999, p-value = 0.5723 # and here we can use another normality test from the &quot;nortest&quot; package called the Anderson-Darling test. ad.test(all_the_water$value) ## ## Anderson-Darling normality test ## ## data: all_the_water$value ## A = 0.60576, p-value = 0.1156 # Point being, there are a number of tests for normality that we can apply. According to the tests you’ve just performed, which datasets ARE and which datesets are NOT normal? 5.12 Deliverable (10 pts, due Thursday 2/2 by 11:59 PM) Write up your answers to HW questions 1 - 5 in a word doc and submit on D2L. "],["lab-2-statistics-in-hydrology-30-pts.html", "Chapter 6 Lab 2: Statistics in hydrology (30 pts) 6.1 Problem 1 (5 pts) 6.2 Problem 2 (5 pts) 6.3 Problem 3 (5 pts) 6.4 Problem 4 (5 pts) 6.5 Problem 5 (5 pts) 6.6 Problem 6 (5 pts) 6.7 Deliverable", " Chapter 6 Lab 2: Statistics in hydrology (30 pts) The repo for this lab can be found here Address each of the questions in the code chunk below and/or by typing outside the chunk (for written answers). 6.1 Problem 1 (5 pts) Load the tidyverse and patchwork libraries and read in the flashy and pine_nfdr datasets. Using the flashy dataset, generate two new dataframes. One for the “WestMnts” and one for the “NorthEast” AGGECOREGION. Name these flashy_west and flasy_ne. Next make pdfs of the average basin rainfall (PPTAVG_BASIN) for the WestMnts (flashy_west) and NorthEast (flashy_ne) agricultural ecoregions. On each pdf add vertical lines showing the mean and median. Label the x axis “Average precipitation (mm)” and the y “Density”. Set the x scale limits to 0 - 500 using xlim(c(0, 500)). Save each ggplot as an object and stack them on top of each other. Provide a caption below the figure that states which is on top, which is on bottom, and which color is the mean and which is the median. # Make the ggplot for the NE # Make the ggplot for the Mountain West Use patchwork to stack the NE and West ggplots. 6.2 Problem 2 (5 pts) Calculate the SD and IQR for precipitation for the MtnsWest and Northeast ag-ecoregions. Using the SD, IQR and density plots from above, comment on the distributions of precipitation for the MtnsWest and Northeast ag-ecoregions. Which has a larger spread? 6.3 Problem 3 (5 pts) Next, make Q-Q plots and perform a Shapiro-Wilk test for normality on the precipitation data sets for the MtnsWest and Northeast ag-ecoregions. Using the results from these tests discuss whether or not the distributions are normal. Also if you based your decision as to whether the data sets were normal on the pdfs you developed in problem 1, the Q-Q test, and Shapiro-Wilk test would each lead you to same conclusion? Please comment. 6.4 Problem 4 (5 pts) Make a plot that shows the distribution of the data from the PINE watershed and the NFDR watershed (two pdfs on the same plot). Log the x axis, label the x axis “Flow (cfs)” and the y axis “Density”. 6.5 Problem 5 (5 pts) You want to compare how variable the discharge is in each of the watersheds in question 4. Which measure of spread would you use and why? If you wanted to measure the central tendency which measure would you use and why? 6.6 Problem 6 (5 pts) Compute 3 measures of spread and 2 measures of central tendency for the PINE and NFDR watershed. (hint: use group_by() and summarize()) Be sure your code outputs the result. Which watershed has higher flow? Which one has more variable flow? How do you know? 6.7 Deliverable Please knit this .Rmd as .html and submit both on D2L. This lab will be due Wednesday 2/8 at 11:59 PM. "],["usgs-dataretrieval-part-1.html", "Chapter 7 USGS dataRetrieval: Part 1 7.1 Goals 7.2 Exploring what dataRetrieval can do 7.3 First, let’s map these sites. 7.4 Joins 7.5 Join practice example 7.6 Now we know what is available so let’s download some data from the Gallatin site using “readNWISdata”.", " Chapter 7 USGS dataRetrieval: Part 1 Readings: Introduction to the dataRetrieval package https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html Chapter 12 &amp; 13 of R for Data Science https://r4ds.had.co.nz/tidy-data.html 7.1 Goals Get familiar with the dataRetrieval package Intro to joins Learn about long vs. wide data and how to change between them Prep question: How would you get data from the USGS (non-R)? Test it out by downloading data for gauge 06050000. Install and load the necessary packages. #install.packages(&quot;dataRetrieval&quot;) #install.packages(&quot;leaflet&quot;) library(dataRetrieval) library(tidyverse) library(lubridate) library(leaflet) 7.2 Exploring what dataRetrieval can do Think about the dataRetrieval as a way to interact with same public data you can access through waterdata.usgs.gov but without having to click on buttons and search around. It makes getting data or doing analyses with USGS data much more reproducible and fast! The documentation for the package is extremely helpful: https://cran.r-project.org/web/packages/dataRetrieval/vignettes/dataRetrieval.html https://waterdata.usgs.gov/blog/dataretrieval/ I always have to look up how to do things because the package is very specialized! This is the case with most website APIs, in my experience. It’s a good argument for getting good at navigating package documentation! Basically you just look through and try to piece together the recipe for what you want to do using the examples they give in the document. First, let’s get some information about the gauging stations using the whatNWISsites(), readNWISsite(), and whatNWISdata() functions. Try each out and see what they tell you. Remember, all the parameter codes and site names get passed to dataRetrieval functions as characters, ao they must be in quotes. Link to parameter codes Let’s use dataRetrieval to see what’s available in Montana. # Load the stream gauge data # Configs that could be changed ... param_cd &lt;- &quot;00060&quot; # Parameter code for streamflow . See `dataRetrieval::parameterCdFile` for more. service_cd &lt;- &quot;dv&quot; # means &quot;daily value&quot;. See `https://waterservices.usgs.gov/rest/` for more info. # State code state_cd &lt;- &quot;MT&quot; # set start and end dates for streamflow time series start = as.Date(&quot;1990-10-01&quot;) end = as.Date(&quot;2022-10-01&quot;) # Use whatNWISsites to identify what gauges exist that meet the above conditions. stream_gauges &lt;- whatNWISsites(parameterCd = param_cd, service = service_cd, stateCd = state_cd, startDate = start, endDate = end) %&gt;% select(site_no, station_nm, dec_lat_va, dec_long_va) Map the gauges using leaflet. mt_gauge_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;Stamen.Terrain&quot;) %&gt;% addAwesomeMarkers(data = stream_gauges, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Start date:&quot;, start, &quot;&lt;br&gt;End date:&quot;, end)) mt_gauge_map Let’s change the popup to be the site_no. mt_gauge_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;Stamen.Terrain&quot;) %&gt;% addAwesomeMarkers(data = stream_gauges, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Site&quot;, site_no)) Challenge: change the code that generates the “stream_gauges” object to generate an object called “gallatin” that only include the gauges within Gallatin County and map them. Investigate the map and pay particular attention to 06048650 and 06048700. What is going on there? gallatin &lt;- whatNWISdata(parameterCd = param_cd, service = service_cd, stateCd = state_cd, startDate = start, endDate = end, countyCd = &quot;Gallatin&quot;) %&gt;% select(site_no, station_nm, dec_lat_va, dec_long_va) gallatin_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;Stamen.Terrain&quot;) %&gt;% addAwesomeMarkers(data = gallatin, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Site&quot;, site_no)) gallatin_map Let’s explore some of dataRetrieval capabilities using two local gauging sites. Let’s work with 1. A USGS gauge on the Gallatin River near Gallatin Gateway. The gague number is 06043500. And another gauge near the reservoir at Hyalite, gauge number 06050000. 7.3 First, let’s map these sites. #important: note the site number gets input as a character site &lt;- c(&quot;06043500&quot;, &quot;06050000&quot;) #Information about the sites site_info &lt;- readNWISsite(site) # plot the gauge sites with a street map background basemap street_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;OpenStreetMap&quot;) %&gt;% addAwesomeMarkers(data = site_info, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Site:&quot;, site_no)) street_map # plot the gauge sites with a topo map background basemap topo_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;Stamen.Terrain&quot;) %&gt;% addAwesomeMarkers(data = site_info, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Site:&quot;, site_no)) topo_map # or with imagery esri_map &lt;- leaflet() %&gt;% addProviderTiles(&quot;Esri.WorldImagery&quot;) %&gt;% addAwesomeMarkers(data = site_info, lat = ~dec_lat_va, lng = ~dec_long_va, label = ~station_nm, popup = ~paste(&quot;Site:&quot;, site_no)) esri_map Now we have looked at where the sites are located. Now let’ see what data are available with whatNWISdata(). #What data are available for the sites? # service = dv means give us the daily averages. data_available &lt;- whatNWISdata(siteNumber = site, service = &quot;dv&quot;, statCd = &quot;00003&quot;) view(data_available) 7.4 Joins When we look at what whatNWISdata returns, we see it gives us parameter codes (param_cd), but doesn’t tell us what they mean. This is a common attribute of databases: you use a common identifier but then have the full information in a lookup file. In this case, the look-up information telling us what the parameter codes mean is in “parameterCdFile” which loads with the dataRetrieval package. So, you could look at that and see what the parameters mean. In the console type “view(parameterCdFile)”. That doesn’t seem very practical. There are over 24,000 rows! Instead let’s have R do it and add a column that tells us what the parameters mean. Enter JOINS! Joins allow us to combine the data from two different data frames (dfs) that have a column in common. At its most basic, a join looks for a matching row with the same key in both dfs (for example, a USGS gauge number) and then combines the rows. So now you have all the data from both dfs, matched on the key. But you have to make some decisions: what if a key value exists in one df but not the other? Do you just drop that observation? Do you add an NA? Let’s look at the different options. Take for example the two dfs, FlowTable and SizeTable. The SiteName values are the key values and the MeanFlow and WSsize values are the data. Join Setup Note River1 and River2 match up, but River3 and River5 only exist in one df or the other. The first way to deal with this is an INNER JOIN: inner_join() In an inner join, you only keep records that match. So the rows for River3 and River5 will be dropped because there is no corresponding data in the other df. See below: Inner Join But what if you don’t want to lose the values in one or the other or both?! For instance, let’s say you have a bunch of discharge data for a stream, and then chemistry grab samples at a weekly or monthly timestep. You want to join the chemistry to the discharge based on the dates and times they were taken. But when you do this, you don’t want to delete all the discharge data where there is no chemistry! We need another option. Enter OUTER JOINS LEFT JOIN, left_join(): Preserves all values from the LEFT df, and pastes on the matching ones from the right. This creates NAs where there is a value on the left but not the right. (this is what you’d want to do in the discharge - chemistry example above) Left Join RIGHT JOIN, right_join(): Preserves all values from the RIGHT df, and pastes on the matching ones from the left. This creates NAs where there is a value on the right but not the left. Right Join FULL JOIN, full_join(): KEEP EVERYTHING! The hoarder of the joins. No matching record on the left? create an NA on the right! No matching value on the right? Create an NA on the left! NAs for everyone! Full Join When you do this in R, you use the functions identified in the descriptions with the following syntax (see example below): if the column is named the same in both data sets &gt; xxx_join(left_tibble, right_tibble, by = “key_column”)** if the column is named differently in both data sets &gt; xxx_join(left_tibble, right_tibble, by = c(“left_key” = “right_key”) Left Join Differing Col Names Note in both of the above, when you specify which column to use as “by” you have to put it in quotes. 7.5 Join practice example In the chunk below write code to add information about the parameters in data_available by joining it with the parameterCdFile. The column with the parameter codes is called parm_cd in data_available and parameter_cd in parameterCdFile data_available_cd &lt;- left_join(data_available, parameterCdFile, by = c(&quot;parm_cd&quot; = &quot;parameter_cd&quot;)) view(data_available_cd) # that made a lot of columns, write code to clean that up and keep only relevant columns. data_avail_clean &lt;- data_available_cd %&gt;% select(site_no, station_nm, parm_cd, srsname, #substance registry services parameter_units, begin_date, end_date) view(data_avail_clean) Question: in data_avail_clean why are there two rows for 06043500? 7.6 Now we know what is available so let’s download some data from the Gallatin site using “readNWISdata”. # first let set some parameters. What do each of these do? gal_q_site &lt;- data_avail_clean$site_no[2] gal_q_param &lt;- data_avail_clean$parm_cd[2] gal_q_start &lt;- data_avail_clean$begin_date[2] gal_q_end &lt;- data_avail_clean$end_date[2] gal_q &lt;- readNWISdata(sites = gal_q_site, parameterCd = gal_q_param, service =&quot;dv&quot;, startDate = gal_q_start, endDate = gal_q_end) view(gal_q) # Those column names are awful. Let&#39;s try that again and pipe into &quot;renameNWISColumns()&quot; gal_q &lt;- readNWISdata(sites = gal_q_site, parameterCd = gal_q_param, service =&quot;dv&quot;, startDate = gal_q_start, endDate = gal_q_end) %&gt;% renameNWISColumns() view(gal_q) tail(gal_q) ## agency_cd site_no dateTime Flow Flow_cd tz_cd ## 34257 USGS 06043500 2024-01-16 247 P UTC ## 34258 USGS 06043500 2024-01-17 280 P UTC ## 34259 USGS 06043500 2024-01-18 301 P UTC ## 34260 USGS 06043500 2024-01-19 298 P UTC ## 34261 USGS 06043500 2024-01-20 310 P UTC ## 34262 USGS 06043500 2024-01-21 304 P UTC head(gal_q) ## agency_cd site_no dateTime Flow Flow_cd tz_cd ## 1 USGS 06043500 1889-08-01 425 A UTC ## 2 USGS 06043500 1889-08-02 426 A UTC ## 3 USGS 06043500 1889-08-03 426 A UTC ## 4 USGS 06043500 1889-08-04 426 A UTC ## 5 USGS 06043500 1889-08-05 426 A UTC ## 6 USGS 06043500 1889-08-06 426 A UTC But is the Q data continuous for the entire period? The easiest way to find out is to make a plot. Make a ggplot with Q on the y and date on the x. gal_q %&gt;% ggplot(aes(x = dateTime, y = Flow)) + geom_point() # Another way to explore this data is to use plotly, which creates an interactive graph. #install.packages #library(plotly) # I&#39;m commenting this section out becuase it bogs down the knitting. #ggplotly(gal_q %&gt;% #ggplot(aes(x = dateTime, y = Flow)) + #geom_point()) # as an fyi, plotly is slow with large data sets. So you might want to comment this section out after you do your exploration. # We&#39;ve identified some gaps. Now let&#39;s filter to everything after 1990 gal_q &lt;- gal_q %&gt;% filter(dateTime &gt; &quot;1990-09-30&quot;) view(gal_q) Let’s now plot that more recent (1990 - now) data. gal_q %&gt;% ggplot(aes(x = dateTime, y = Flow)) + geom_point() That looks better! Challenge follow the steps from above to download and plot temperature data for the Gallatin River site (06043500) from begin_date to end_date. call the data frame gal_t. gal_t_site &lt;- data_avail_clean$site_no[1] gal_t_param &lt;- data_avail_clean$parm_cd[1] gal_t_start &lt;- data_avail_clean$begin_date[1] gal_t_end &lt;- data_avail_clean$end_date[1] gal_t &lt;- readNWISdata(sites = gal_t_site, parameterCd = gal_t_param, service =&quot;dv&quot;, startDate = gal_t_start, endDate = gal_t_end) %&gt;% renameNWISColumns() %&gt;% select(site_no, dateTime, Wtemp_Max, Wtemp_Min, Wtemp) view(gal_t) gal_t %&gt;% ggplot(aes(x = dateTime, y = Wtemp)) + geom_point() Challenge use join to create a dataframe called gal_t_q that joins gal_t and gal_q and keeps only Q data when there is T data. Then plot both Q and T and stack the plots. gal_t_q &lt;- left_join(gal_t, gal_q, by = &quot;dateTime&quot;) view(gal_t_q) #let&#39;s clean that up! head(gal_t_q) ## site_no.x dateTime Wtemp_Max Wtemp_Min Wtemp agency_cd ## 1 06043500 2001-05-04 10.0 4.0 7.0 USGS ## 2 06043500 2001-05-05 9.0 5.5 7.5 USGS ## 3 06043500 2001-05-06 8.5 3.5 6.5 USGS ## 4 06043500 2001-05-07 9.5 3.5 7.0 USGS ## 5 06043500 2001-05-08 9.5 6.0 8.0 USGS ## 6 06043500 2001-05-09 8.5 6.0 7.5 USGS ## site_no.y Flow Flow_cd tz_cd ## 1 06043500 777 A UTC ## 2 06043500 878 A UTC ## 3 06043500 935 A UTC ## 4 06043500 917 A UTC ## 5 06043500 1030 A UTC ## 6 06043500 1240 A UTC gal_t_q &lt;- gal_t_q %&gt;% select(site_no.x, dateTime, Wtemp, Flow) %&gt;% rename(site_no = 1) view(gal_t_q) head(gal_t_q) ## site_no dateTime Wtemp Flow ## 1 06043500 2001-05-04 7.0 777 ## 2 06043500 2001-05-05 7.5 878 ## 3 06043500 2001-05-06 6.5 935 ## 4 06043500 2001-05-07 7.0 917 ## 5 06043500 2001-05-08 8.0 1030 ## 6 06043500 2001-05-09 7.5 1240 tail(gal_t_q) ## site_no dateTime Wtemp Flow ## 1240 06043500 2004-09-24 9.0 512 ## 1241 06043500 2004-09-25 9.0 473 ## 1242 06043500 2004-09-26 9.5 452 ## 1243 06043500 2004-09-27 10.0 453 ## 1244 06043500 2004-09-28 9.5 438 ## 1245 06043500 2004-09-29 8.5 429 p1 &lt;-gal_t_q %&gt;% ggplot(aes(x = dateTime, y = Flow)) + geom_point(color = &quot;blue&quot;) + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;) + theme_linedraw() p2 &lt;- gal_t_q %&gt;% ggplot(aes(x = dateTime, y = Wtemp)) + geom_point(color = &quot;red&quot;) + labs(x = &quot;&quot;, y = &quot;T (degrees C)&quot;) + theme_linedraw() library(patchwork) p2 / p1 Challenge Using gal_t_q, plot Q in cubic meters per second on x and T in Celsius on Y, color by date, add labels to axes, and select a theme. gal_t_q %&gt;% mutate(q_cms = Flow * 0.0283) %&gt;% ggplot(aes(x = q_cms, y = Wtemp, color = dateTime)) + geom_point() + labs(x = &quot;Q (cms)&quot;, y = &quot;T (degrees C)&quot;, color = &quot;Date&quot;) + theme_linedraw(base_size = 16) Ok. So above was one way that we could download some data. Let’s explore another approach. # Let&#39;s download some dta for the Hyalite Creek and Gallatin Gateway gauges. Let&#39;s also use the parameters that we have already defined. Things like servie_cd, param_cd, site, etc. hy_gal &lt;- readNWISdata(site = site, service = service_cd, parameterCd = param_cd, startDate = start, endDate = end) %&gt;% renameNWISColumns() %&gt;% select(site_no, date = dateTime, flow = Flow) head(hy_gal) ## site_no date flow ## 1 06043500 1990-10-01 375 ## 2 06043500 1990-10-02 379 ## 3 06043500 1990-10-03 376 ## 4 06043500 1990-10-04 383 ## 5 06043500 1990-10-05 383 ## 6 06043500 1990-10-06 421 tail(hy_gal) ## site_no date flow ## 15814 06050000 2022-09-26 49.8 ## 15815 06050000 2022-09-27 49.8 ## 15816 06050000 2022-09-28 49.7 ## 15817 06050000 2022-09-29 49.6 ## 15818 06050000 2022-09-30 49.6 ## 15819 06050000 2022-10-01 49.9 # now let&#39;s do a join so that we have both the site number (site_no) and the station name in the df. hy_gal_names &lt;- left_join(hy_gal, data_avail_clean, by = &quot;site_no&quot;) %&gt;% select(station = station_nm, site_no, date, flow) head(hy_gal_names) ## station site_no date ## 1 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-01 ## 2 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-01 ## 3 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-02 ## 4 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-02 ## 5 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-03 ## 6 Gallatin River near Gallatin Gateway, MT 06043500 1990-10-03 ## flow ## 1 375 ## 2 375 ## 3 379 ## 4 379 ## 5 376 ## 6 376 tail(hy_gal_names) ## station site_no date ## 27503 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-09-26 ## 27504 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-09-27 ## 27505 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-09-28 ## 27506 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-09-29 ## 27507 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-09-30 ## 27508 Hyalite C at Hyalite R S nr Bozeman MT 06050000 2022-10-01 ## flow ## 27503 49.8 ## 27504 49.8 ## 27505 49.7 ## 27506 49.6 ## 27507 49.6 ## 27508 49.9 Now let’s make a plot of Q at the Gallatin and Hyalite gauges to do an assessment (EDA) of the data. What is your observation? hy_gal_names %&gt;% ggplot(aes(x = date, y = flow, color = station)) + geom_point(alpha = 0.5) + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;, color = &quot;Gauge&quot;) + theme_linedraw() # We can log the y axis to zoom in on the low values a bit. hy_gal_names %&gt;% ggplot(aes(x = date, y = flow, color = station)) + geom_point(alpha = 0.5) + labs(x = &quot;Date&quot;, y = &quot;Q (cfs)&quot;, color = &quot;Gauge&quot;) + theme_linedraw() + scale_y_log10() Challenge Download Q data for all of the sites in the gallatin df. Rename columns using renameNWIScolumns and call this df gallatin_q. Join gallatin and gallatin_q so that you have a df that includes the station names. Plot Q and color by station name. gallatin_sites &lt;- gallatin$site_no gallatin_q &lt;- readNWISdata(sites = gallatin_sites, parameterCd = param_cd, service = &quot;dv&quot;, startDate = start, endDate = end) gallatin_q &lt;- gallatin_q %&gt;% renameNWISColumns() %&gt;% select(site_no, date_time = dateTime, flow = Flow) gallatin_q_names &lt;- left_join(gallatin_q, gallatin, by = &quot;site_no&quot;) %&gt;% rename(flow_cfs = flow) gallatin_q_names %&gt;% ggplot(aes(x = date_time, y = flow_cfs, color = station_nm)) + geom_point() + facet_wrap(facets = &quot;station_nm&quot;) "],["lab-3---dataretrieval-and-area-normalized-annual-flows-20-pts.html", "Chapter 8 Lab 3 - dataRetrieval and area normalized annual flows (20 pts) 8.1 Summary (20 pts) 8.2 Deliverable", " Chapter 8 Lab 3 - dataRetrieval and area normalized annual flows (20 pts) Lab 3 repo here In this lab we are going to use dataRetrieval and other packages (e.g., tidyverse, leaflet, lubridate) to download and analyze flow data. You are going to download streamflow data for gauges in Park County Montana. You will start by exploring how many gauges there are in the county and mapping them to see where they are. Next you will download the data from the gauges and do some EDA. You will then do some data analysis on a subset of gauges that meet certain criteria. Load the necessary packages. Use dataRetrieval to see how many USGS streamflow gauges there are in Park County that have data from 10/1/1990 to 10/1/2022. Call this df “stream_gauges” # write the csv to save the data locally. This means you don&#39;t have to re-download everytime you work with this. #write_csv(stream_gauges, &quot;stream_gauges.csv&quot;) # then load the data. #stream_gauges &lt;- read_csv(&quot;stream_gauges.csv&quot;) # you will want to repeat this process in any chunks that have dataRetrieval functions that are reaching out to the internet. Map the gauges. Download the Q data for the gauges using dataRetrieval. Call the dataframe you create “flows” # Again we want to write and then read the df so we can comment out the dataRetrieval function from trying to go to the internet. #write_csv(flows, &quot;flows.csv&quot;) #flows &lt;- read_csv(&quot;flows.csv&quot;) Make a facet wrap to look at the flows for each gauge. Filter your data frame of streamflow data to only include gauges that have data that extends back to 1990. Hint: you should have 4 gauges. Call this df “flows_filt” Make another facet wrap of flow at those 4 guages. For the 4 gauges you have kept convert flow from cfs to mm/day. You will need to know the area of each watershed, which you can get from readNWISsite. Remember that your Q data are currently in cfs so there are multiple conversions you will need to do in order to get to mm/day. To add drainage area to you df you are going to need to use join(). You might need some intermediate dfs but in the end you will have a df that has flow in mm/day. Call this df “flow_mm”. Make a facet wrap of flow for each gauge in mm/day. Compute and plot total annual flow for each gauge. You will need lubridate and year(). Call this df “flow_mm_an” Make a facet wrap of total annual flow in mm per year for each gauge. Plot total annual flow from each gauge in mm/year all on the same graph. Make a map of the gauges to aid in interpretation of patterns in total annual flows. 8.1 Summary (20 pts) Using mm/year allows us to compare across gauges with very different volumetric (e.g., cfs or cms) flow rates. Use your plots of mm/year to comment on: 1. Does one gauge produce consistently less flow year to year? 2. Does one gauge produce consistently more flow year to year? 3. Which gauges show similar patterns in annual water yield? Provide a plausible explanation for why that might be. 4. For any gauges that seem different than the others, provide a plausible explanation for why that might be. 8.2 Deliverable Knit this .Rmd to .html and upload both on D2L. To get this to knit you will need to save objects as csv (write_csv) then load the csv (read_csv) so that you can comment out the whatNWISsites, readNWISdata, etc lines of code. If those lines are not commented out, this will likely not knit. "],["demo-dataretrieval-and-streamstats.html", "Chapter 9 Demo: dataRetrieval and StreamStats 9.1 Load some standard packages. 9.2 Define parameters for dataRetrieval. 9.3 We can look for sites based on state, county, huc, site number, etc. 9.4 We can use dataRetrieval to look for water quality data in the Water Quality Portal (WQP) and we can search for groundwater data. 9.5 StreamStats 9.6 Lab 3 Demo", " Chapter 9 Demo: dataRetrieval and StreamStats This is a demo of some things we can do with dataRetrieval and StreamStats. 9.1 Load some standard packages. library(tidyverse) library(dataRetrieval) library(leaflet) 9.2 Define parameters for dataRetrieval. param_cd &lt;- &quot;00060&quot; service_cd &lt;- &quot;dv&quot; state_cd &lt;- &quot;MT&quot; county_cd &lt;- &quot;Park County&quot; start &lt;- as.Date(&quot;1990-10-01&quot;) end &lt;- as.Date(&quot;2022-10-01&quot;) 9.3 We can look for sites based on state, county, huc, site number, etc. stream_gauges &lt;- whatNWISsites(parameterCd = param_cd, service = service_cd, stateCd = state_cd, countyCd = county_cd, startDate = start, endDate = end) stream_gauges_info &lt;- whatNWISdata(parameterCd = param_cd, service = service_cd, stateCd = state_cd, countyCd = county_cd, startDate = start, endDate = end) %&gt;% filter(begin_date &lt; start, end_date &gt; end) huc_cd &lt;- &quot;10020008&quot; # This is our local hydrologic unit code. huc_gauges &lt;- whatNWISsites(parameterCd = param_cd, service = service_cd, huc = huc_cd, startDate = start, endDate = end) 9.4 We can use dataRetrieval to look for water quality data in the Water Quality Portal (WQP) and we can search for groundwater data. #wq_sites &lt;- whatWQPsites(huc = huc_cd, startDate = start, endDate = end, characteristicName = &quot;Nitrate&quot;) #wq_sites &lt;- whatWQPsites(paste0(&quot;USGS-&quot;, site = &quot;06043500&quot;), startDate = start, endDate = end, characteristicName = &quot;Nitrate&quot;) #gw_sites &lt;- whatNWISdata(parameterCd = &quot;72019&quot;, stateCd = state_cd, countyCd = county_cd) 9.5 StreamStats StreamStats isn’t available through the Comprehensive R Archive Network (CRAN), so we have to install from GitHub using devtools. Streamstats also requires the sf library. #install.packages(&quot;devtools&quot;) #devvtool::install_github(&quot;markwh/streamstats&quot;) #install.packages(&quot;sf&quot;) #library(sf) #library(streamstats) Here are some links to documentation about StreamStats: https://github.com/markwh/streamstats https://streamstats.usgs.gov/docs/streamstatsservices/#/ https://rdrr.io/github/markwh/streamstats/man/ https://www.usgs.gov/streamstats And here is a link to a Stack Overflow about writing a function to delineate many watersheds using the StreamStats package. https://stackoverflow.com/questions/69294130/how-can-i-delineate-multiple-watersheds-in-r-using-the-streamstats-package 9.5.1 Delineate one watershed in StreamStats Here I am just getting the lat and long for the Gallatin Gateway USGS gauge. I only want one row of data so I’ve put in a parameter code and a service code. If I didn’t do that it would give me lots of rows (1 for every type of measurement made at this gauge). gallatin_gauge_loc &lt;- whatNWISdata(site = &quot;06043500&quot;, parameterCd = &quot;00060&quot;, service = &quot;dv&quot;) gall_gauge &lt;- whatNWISdata(site = &quot;06043500&quot;) gall_x &lt;- gallatin_gauge_loc$dec_long_va gall_y &lt;- gallatin_gauge_loc$dec_lat_va The delineateWatershed function will do the work. #gallatin_gauge_map &lt;- delineateWatershed(xlocation = gall_x, ylocation = gall_y, crs = 4326, includeparameters = &quot;true&quot;, includeflowtypes = &quot;true&quot;) #leafletWatershed(gallatin_gauge_map) Other statistics can be found using the computeChars (for watershed characteristics such as basin area and land-use) and computeFlowStats (for statistics such as flow percentiles) #chars &lt;- computeChars(workspaceID = gallatin_gauge_map$workspaceID, rcode = &quot;MT&quot;) # chars is a list so we can make that easier to work with by grabbing some information from that list and putting it into a tibble (i.e., a dataframe) #basin_chars &lt;- chars$parameters %&gt;% #as_tibble() # stats gives us flow statistics for the site. #stats &lt;- computeFlowStats(workspaceID = gallatin_gauge_map$workspaceID, rcode = &quot;MT&quot;, simplify = TRUE) 9.6 Lab 3 Demo This bit is just some code for lab 3 to help with getting a column of drainage area in your flow df. #drain &lt;- readNWISsite(stream_gauges$site_no) %&gt;% # select(site_no, drain_area_va) #flows_filt_new &lt;- left_join(flows_filt, drain, by = &quot;site_no&quot;) "],["exam-1-units-1---4.html", "Chapter 10 Exam 1: Units 1 - 4", " Chapter 10 Exam 1: Units 1 - 4 Repo for exam 1, covering units 1-4. This “exam” is much like our labs, the only difference is that in this lab you will combine the skills you have learned in labs 1 - 3. The previous labs and the class bookdown will be a very valuable resource to you in completing this project. I don’t want you spending huge amounts of time on this. In theory you should be able to move through this in an hour or two. If it is taking you longer than that or if you get stuck on something just reach out to me. "],["demo-on-calculating-flow-totals-in-mm..html", "Chapter 11 Demo on calculating flow totals in mm.", " Chapter 11 Demo on calculating flow totals in mm. library(dataRetrieval) library(tidyverse) site &lt;- &quot;06187915&quot; parm_cd &lt;- &quot;00060&quot; service_cd &lt;- &quot;dv&quot; start &lt;- as.Date(&quot;2022-05-01&quot;) end &lt;- as.Date(&quot;2022-10-01&quot;) gauge_info &lt;- readNWISsite(site) %&gt;% select(site_no, station_nm, drain_area_va) flow &lt;- readNWISdata(sites = site, parameterCd = parm_cd, service = service_cd, startDate = start, endDate = end) %&gt;% renameNWISColumns() %&gt;% select(site_no, date = dateTime, flow_cfs = Flow) flow &lt;- left_join(flow, gauge_info, by = &quot;site_no&quot;) flow_mm &lt;- flow %&gt;% mutate(cfd = flow_cfs * 86400) %&gt;% mutate(cmd = cfd * 0.0283168) %&gt;% mutate(drain_sq_me = drain_area_va * 2.59e+6) %&gt;% mutate(m_d = cmd/drain_sq_me) %&gt;% mutate(mm_d = m_d * 1000) %&gt;% select(site_no, station_nm, date, mm_d) flow_mm %&gt;% ggplot(aes(x = date, y = mm_d)) + geom_point() library(lubridate) flow_tot_an &lt;- flow_mm %&gt;% mutate(year = year(date)) %&gt;% group_by(year, site_no, station_nm) %&gt;% summarize(tot_an = sum(mm_d, na.rm = TRUE)) "],["supplemental-chapter-on-field-methods-in-fluvial-geomorphology.html", "Chapter 12 Supplemental chapter on field methods in fluvial geomorphology 12.1 Overall Learning Objectives: 12.2 Lecture 12.3 Stream Cross Section and Gradient 12.4 Wolman Pebble Count 12.5 Manning’s Roughness Coefficient 12.6 Rosgen Stream Classification 12.7 Relative Bed Stability (RBS)", " Chapter 12 Supplemental chapter on field methods in fluvial geomorphology The term “fluvial” references running waters, “geo” refers to earth, and “morphology” refers to channel shape. Thus, fluvial geomorphology is the study of stream form and function as well as the interactions between streams and the surrounding landscape. Stream systems are dynamic meaning they physically respond to upstream inputs of streamflow, sediment, and debris. In this module, we will introduce how to measure a stream slope and cross section, determine a Manning’s roughness coefficient, classify a stream based on the Rosgen Classification system, quantify the grain size distribution of sediment with the Wolman pebble count, and calculate the relative bed stability (RBS). 12.1 Overall Learning Objectives: At the end of this module, students will understand the key principles of fluvial geomorphology including defining characteristics of a channel cross section, determining a Manning’s roughness coefficient, classifying a stream based on the Rosgen Classification system, quantifying the grain size distribution of sediment with the Wolman pebble count method, and using these measurements to determine the relative bed stability. 12.2 Lecture 12.3 Stream Cross Section and Gradient Field Cross Section Methods: A stream system is not defined by only the main, active channel but also how that channel interacts with the surrounding landscape. Because of this, we also take note of the bankfull location and the flood-prone area when assessing a stream cross section. The bankfull location is the water level at which the stream begins to spill out of the main channel into the active floodplain. Bankfull flow has a recurrence interval of ~1.5 years, although this varies and is debated. Bankfull flows tend to move most of the sediment over time and are largely responsible for shaping the stream channel. Bankfull locations can be visually observed in streams where there are changes in slope, vegetation and particle size. The flood prone area is defined by the width at two times the maximum bankfull depth. Frequency analysis of historic data suggests that the floodprone area is inundated at a 50 year recurrence interval. Stream gradient is another important control on channel morphology. The gradient of a river is a measure of drop in elevation of a stream per unit horizontal distance (i.e. slope). Generally, steeper gradients are associated with faster flowing water and more erosive potential. Gradient can be measured using a clinometer and stadia rod over a short (e.g. 10 m) stretch of the river. See supplemental video here: Overall, the observer with the clinometer first determines the point on the stadia rod that reads 0 slope while the stadia rod is held nearby with the bottom set on the channel bed. Then their teammate measures 10 meters downstream and holds the stadia rod against the channel bed. The observer, who has remained in the same location, looks at the zero point on the stadia rod with one eye and records the slope reading on the clinometer. Generally, 10 meters is about the limit of how far people can accurately read the stadia rod, so in order to get slopes along 100 m of stream reach the team would simply repeat this process 10 times. To get the water level gradient, follow the same procedure but hold the stadia rod so the bottom is even with the water surface as opposed to holding it on the channel bed. 12.4 Wolman Pebble Count Field Wolman Pebble Count Method: The grain size distribution of a streambed and banks influences the channel form and hydraulics, erosion rates, sediment supply, and other parameters. For example, steep mountain streams with boulders and cobbles are different than low-gradient streams with beds of sand or silt. Land management activities can shift sediment delivery or change streamflow which can lead to cumulative impacts to the aquatic ecosystem. Whether aggradation (i.e. accumulation of sediment) or degradation (i.e. removal of sediment) occurs with this shift is a function of the balance between stream power and sediment supply as defined by Lane’s Balance. Lane’s balance diagram (Figure 4) demonstrates how the channel may respond to a change in various parameters, such as sediment load, channel geometry, channel slope, erosional resistance, and discharges (hydrologic load). For example, by increasing the amount of sediment load the left side of the balance will lower and the scale will tip toward aggradation (sediment deposition); to bring the scale back in balance a change in either the channel geometry, slope and/or hydrologic load would be needed. To quantify grain size distribution, in the field, we apply the Wolman pebble count method using a gravelometer. The observer walks from bankfull to bankfull, collecting material and recording the diameter of the channel material. The observer should remain unbiased during collection. The best way to achieve this is by averting your gaze and picking up the first particle touched by the tip of index finger as you bend over. Determine the smallest slot on the gravelometer that the particle fits through to determine the grain size and record that value. Measure embedded particles or those too large to be moved with the measurement increments on the side of the gravelometer or using a folding ruler. Continue this procedure until you have 100 or more measurements. 12.5 Manning’s Roughness Coefficient Physical properties of a stream such as water depth, channel cross sectional area, stream gradient and channel material can also be applied to Manning’s equation to make estimates of discharge at a specific water depth. Manning’s equation (equation 1) calculates the average velocity, V, of uniform flow in an open channel based on channel properties: where R is the hydraulic radius (L), S is the channel slope (L/L), n is Manning’s roughness coefficient, and k is a unit conversion factor (k = 1 if R is in meters and V is m/s; k = 1.49 if R is in feet and V in ft/s). The hydraulic radius is computed as the cross-sectional area, divided by the wetted perimeter, P. In natural streams the hydraulic radius (R) and depth are nearly identical. Manning’s roughness, n, is a factor that characterizes the channel resistance. There are numerous methods to estimate n, but in this lesson we will focus on comparative (photos) and analytical (Cowan’s) methods. Finally, discharge is calculated by multiplying V by the channel cross-sectional area, A (L2). This is particularly useful when estimating streamflow at bankfull depth or during a flood event when it isn’t safe to conduct direct measurements (i.e., wade across the stream to perform a velocity-area measurement). Manning’s roughness coefficient (n) is commonly determined in the field by the comparative and analytical methods. See supplemental video here: See supplemental reading here: Limerinos online calculator To use the comparative method, photographs of channel segments for which n values have been verified can be used as a comparison standard to aid in assigning n values to similar channels. Refer to the USGS and USFS guides for selecting roughness coefficients: USGS Guide USFS Guide Alternatively, Cowan (1956) developed an analytical procedure for estimating the effects of factors affecting channel roughness (i.e. channel shape and material) to determine the value of n for a channel. The value of n may be computed by equation 2. To apply this equation, refer to Tables 1 &amp; 2 found in the USGS “Guide for selecting roughness coefficients” (link here:) 12.6 Rosgen Stream Classification River scientists often use this framework to classify complex natural river systems into groups that share common physical characteristics. The use of this consistent language allows for effective communication of stream geomorphology between scientific fields. To classify a stream reach correctly, you must complete a hierarchical assessment of channel morphology that includes assessing single or multi-thread channel, entrenchment ratio, width to depth ratio, sinuosity, slope, and grain size (Figure 2). You will use your measurements from section 1 to classify your local field site. Sinuousity video: Supplemental reading on the Rosgen Classififcation System 12.7 Relative Bed Stability (RBS) Relative bed stability (RBS) is a measure of bed stability, or how frequently sediment is mobilized. In streams that have high stability, the sediment is rarely mobilized. In streams with low stability, the sediment is frequently mobilized even at low flows. In a natural stream that is not influenced by disturbance, the bed stability tends toward a moderate value where the bed isn’t too stable or too unstable. Human modification of the landscape (land use and land cover change), human modification of hydrologic regimes (e.g., damming), or natural disturbances like wildfire can alter the balance between flow and sediment supply and thus impact bed stability. For example, downstream of dams the river tends to be sediment starved and the bed often becomes “armored”, meaning that only extremely high flows will mobilize sediment. Conversely, wildfire can deliver large amounts of small sediment (e.g., sand) to the stream channel such that the stream bed becomes extremely mobile. As such, RBS can be used to help hydrologists and geomorphologists assess stream condition and can be used as an indicator of land use, land cover, and water resource management (i.e., damming) impacts on river ecosystems. To calculate RBS, you will use the data collected in sections 1.1-1.4: A LRBS (log of RBS) value of 0 indicates a stream where flow and sediment supply are balanced. As values become more negative, they indicate excess sediment. Conversely, positive values indicate a lack of sediment supply and bed armoring. Reading on using RBS to determine human impacts on streams "],["lab-4---rating-curve-analysis.html", "Chapter 13 Lab 4 - Rating curve analysis", " Chapter 13 Lab 4 - Rating curve analysis https://github.com/tpcovino/lab_4 "],["hydrographs-and-rating-curves.html", "Chapter 14 Hydrographs and rating curves 14.1 Overall Learning Objectives 14.2 Reading 14.3 Lecture 14.4 Optional activity to gain deeper understanding of rating curve 14.5 Summary questions", " Chapter 14 Hydrographs and rating curves A stream’s volumetric flow rate, hereafter referred to as discharge, is a parameter of interest for scientists and practitioners in various areas of hydrology. However, producing a continuous record of discharge (i.e., hydrograph) involves obtaining a continuous record of stage (i.e., water depth), making discharge measurements over a range of stages, establishing and maintaining a relation between the stage and discharge, and applying the stage-discharge relation to the stage record. A hydrograph can provide insight into the primary drivers of flow (i.e., rain vs snow) and can be tracked to ensure users have access to their water rights as well as be used in flooding risk assessments. 14.1 Overall Learning Objectives At the end of this module, students should be able to describe hydrographs of rain vs snow dominated systems, create a stage-discharge relationship, and understand the different methods used to measure streamflow. 14.2 Reading Dingman Appendix F 14.3 Lecture 14.3.1 Hydrographs and Rating Curves A hydrograph is a time series of stream discharge at a specific point in a river. Hydrographs are constructed by continuously measuring stage (depth of water) and developing a rating curve (Figure 1), which is a relationship between stage (D) and discharge (Q) at a specific monitoring location. Discharge can be measured at stream gaging stations using the velocity-area or dilution gauging methods, or by installing a weir or flume. Weirs and flumes have specific geometry and a known relationship between stage and discharge. However, we often don’t have a weir or flume, and we have to construct this relationship by taking measurements of stage and discharge across a range of flows to develop the rating curve. The stage-discharge relationship typically takes the form of a power law equation (Equation 1) and is controlled by channel morphology. By using this mathematical relationship between stage and discharge we can convert a continuous stage record to a continuous discharge record (Figure 2). As noted above, the stage-discharge relationship depends on the characteristics of the stream channel. Therefore, if the channel geometry changes as a result of erosion or deposition, the rating curve needs to be updated. Because of this, it would be ideal to take stage measurements at a control structure (i.e., weirs and flumes) or stable cross section (i.e., at bridges or where bedrock is confining the channel). However, this is not always possible and we must rebuild rating curves if there is significant change in channel morphology. Additionally, it is important not to extrapolate the stage-discharge relationship for data larger or smaller than the stages and discharges measured in the field that were used to create your rating curve. Extrapolating beyond the bounds of the rating curve leads to large uncertainty in the estimates discharge. 14.3.2 Rain versus Snow dominated systems The shape of the hydrograph contains information about the system that you are working in. The most common conclusion that can be drawn from a hydrograph is whether a system is snow or rain dominated and in what hemisphere the stream is located. Snowmelt dominated streams in the northern hemisphere have peak flows in April or May, whereas snowmelt dominated systems in the southern hemisphere will typically have peak flows around October. In the northern hemisphere and indeed in the US, hydrologists prefer to conduct analyses based on the water year (October 1 - September 30) as opposed to the calendar year. This allows the comparison of incoming precipitation and outgoing streamflow, and specifically ensures that snow delivered in October-December is accounted for in the same time period that it is likely to melt, which may be in spring or summer of the following calendar year. A snow dominated hydrograph (Figure 3) typically has a prominent peak in discharge during the spring and summer months driven by snowmelt as temperatures warm. Alternatively, a rain dominated hydrograph is characterized by high magnitude, short duration increases in flow due to specific rain events (Figure 4). These temporal patterns of high and low flows are referred to collectively as a river’s flow regime. The flow regime plays a key role in regulating geomorphic processes that shape river channels and floodplains, ecological processes that govern the life history of aquatic organisms, and is a major determinant of the biodiversity found in river ecosystems. 14.3.3 Making Stage Measurements Stage, or water depth, measurements can be continuously collected with either analog methods (a chart recorder with float gauge) or digital methods (pressure transducers or capacitance rods). These instruments are commonly placed in a stilling well to reduce noise induced by waves. This continuous measurement is usually accompanied with a staff gage which is used to take point measurements on site by a technician. Often control structures are utilized when routinely taking stage measurements at a specific cross section. Control structures such as flumes and weirs are advantageous as they slow stream water, create a smooth surface to measure stage and have a well-known, unchanging geometry. Flumes are self-cleaning whereas weirs are not. Weirs tend to create a pond upstream of the structure. USGS Gaging Station Video: Montana DNRC Video: 14.3.4 Applications of streamflow and stream gage Stage and streamflow are important for a variety of applications. Check out this USGS video describing applications of how the data collected at USGS stream gaging sites is used: 14.4 Optional activity to gain deeper understanding of rating curve Similar to a stage-discharge relationship you would make in a stream, you can make a volume-depth relationship with any cup or bowl at your house. Materials: 2 different water vessels (i.e. cups, water bottles and bowls). Make sure they vary in size and shape. Measuring cup Ruler Notebook Using the measuring cup, add a known amount of water into one of the water vessels. Measure the height of the water with your ruler. Record the total amount of water in the cup and depth of the water. Repeat steps 1 – 3, 5 times. Be sure you have a large range of water depths ranging from almost empty to full. Repeat the above steps with your second water vessel. Plot your results for each water vessel separately. Put the depth of water on the x-axis and the volume on the y-axis. Add a best fit line to your plot. Include these two plots in your Assessment submission. Reflect: What are the differences between your two plots? What is the primary factor controlling the difference between the two volume-stage relationships you developed in the hands on activity? Activity Video: 14.5 Summary questions You have a rating curve that has good coverage between 1 and 7 ft depth. Is it appropriate to estimate discharge for a stage of 8.5 ft with this rating curve? If so, why? If not, why not? Describe the process to create a rating curve at a stream site be sure to include information on: -The requirements of a good cross section to establish the site. -When and how frequently you would make discharge measurements. - What instrumentation would you use to collect continuous stage measurements. "],["flow-duration-curves.html", "Chapter 15 Flow Duration Curves 15.1 Get data 15.2 Review: describe the distribution 15.3 ECDFs 15.4 Calculate flow exceedence probabilities 15.5 Plot a Flow Duration Curve using the probabilities 15.6 Example use of an FDC 15.7 Compare to a boxplot of the same data 15.8 Challenge: Examining flow regime change at the Grand Canyon 15.9 That’s it! Next we will apply some of these principles to look at low-flow statistics.", " Chapter 15 Flow Duration Curves Alright team. So far we have learned to wrangle data, make plots, and look at data distributions. Now it is time to put all that knowledge to use. We are on our way to doing analyses of extreme discharge events: low flow statistics and floods. But in order to do that, we need to understand a common way to look at data distributions in hydrology: the flow duration curve. As you’ll see below, this is basically just a different way of looking at a pdf, and it can take some getting used to. But it is also a very useful tool! As always let’s load the packages we will use: tidyverse, dataRetrieval, lubridate, and patchwork. Patchwork will help us make a multi-panel graph in the last part of the exercise. We will also use theme_set() in this chunk so we don’t have to change the ggplot theme every time we make a plot. library(tidyverse) library(dataRetrieval) library(lubridate) library(patchwork) #set plot theme for the document so we #don&#39;t have to do it in every plot theme_set(theme_classic()) 15.1 Get data To start, let’s grab the USGS discharge data for the gage in Linville NC from 1960 to 2020. We will download the data using USGS dataRetrieval and look at a line plot. siteno &lt;- &quot;02138500&quot; #Linville NC startDate &lt;- &quot;1960-01-01&quot; endDate &lt;- &quot;2020-01-01&quot; parameter &lt;- &quot;00060&quot; Qdat &lt;- readNWISdv(siteno, parameter, startDate, endDate) %&gt;% renameNWISColumns() #Look at the data Qdat %&gt;% ggplot(aes(x = Date, y = Flow))+ geom_line() 15.2 Review: describe the distribution Make a plot to view the distribution of the discharge data. What is the median flow value? What does this tell us about flow at that river? How often is the river at or below that value? Could you pick that number off the plot? What about the flow the river is at or above only 5% of the time? Qdat %&gt;% ggplot(aes(Flow))+ stat_density()+ scale_x_log10()+ geom_vline(xintercept = median(Qdat$Flow), color = &quot;red&quot;) 15.3 ECDFs Let’s look at an Empirical Cumulative Density Function (ECDF) of the data. Look at this carefully, what does it show? How is it different from the pdf of the data? Plot the median again. Without the line on the plot, how would you tell where the median is? Given your answer to the question above, can you determine the flow the river is at or above only 25% of the time? Think carefully about what the y axis of the ECDF means. Qdat %&gt;% ggplot(aes(Flow))+ stat_ecdf()+ scale_x_log10()+ geom_vline(xintercept = median(Qdat$Flow), color = &quot;red&quot;)+ geom_vline(xintercept = quantile(Qdat$Flow)[4], color = &quot;blue&quot;) 15.4 Calculate flow exceedence probabilities In hydrology, it is common to look at a similar representation of flow distributions, but with flow on the Y axis and “% time flow is equaled or exceeded” on the X axis. There are a number of ways we could make this plot: for example we could transform the axes of the plot above or we could use the function that results from the ECDF function in R to calculate exceedence probabilities at flow throughout our range of flows. But for our purposes, we are just going to calculate it manually. We are going to calculate our own exceedence probabilities because knowing how to do this will help us understand what a flow duration curve is AND we will need to do similar things in our high and low flow analyses. The formula for exceedence probability (P) is below. What do we need to calculate this? Exceedence probability (P), Probability a flow is equaled or exceeded \\(P = 100 * [M / (n + 1)]\\) M = Ranked position of the flow n = total number of observations in data record Here’s a description of what we will do: &gt; Pass our Qdat data to mutate and create a new column that is equal to the ranks of the discharge column. &gt; Then pass that result to mutate again and create another column equal exceedence probability (P) * 100, which will give us %. #Flow is negative in rank() to make #high flows ranked low (#1) Qdat &lt;- Qdat %&gt;% mutate(rank = rank(-Flow)) %&gt;% mutate(P = 100 * (rank / (length(Flow + 1)))) 15.5 Plot a Flow Duration Curve using the probabilities Now construct the following plot: A line with P on the x axis and flow on the y axis. Name the x axis “% Time flow equaled or exceeded” and log the y axis. That’s a flow duration curve! Questions about the flow duration curve: * How often is a flow of 100 cfs exceeded at this gage? * Is flow more variable for flows exceeded 0-25% or of the time or 75-100% * of the time? * How can you tell? Qdat %&gt;% ggplot(aes(x = P, y = Flow))+ geom_line()+ scale_y_log10()+ xlab(&quot;% Time flow equalled or exceeded&quot;)+ ylab(&quot;Q (cfs)&quot;) 15.6 Example use of an FDC Let’s explore one potential use of flow duration curves: examining the differences between two sets of flow data. From the line plot of the discharge, it looked like the flow regime may have shifted a bit in the data between the early years and newer data. Let’s use flow duration curves to examine potential differences. We can come up with groups and then use group_by to run the analysis by groups instead of the whole dataset. We are introducing a new function here called case_when(). This allows you to assign values to a new column based on values in another column. In our case, we are going to name different time periods in our data. We will then group the data by these periods and calculate exceedence probabilities for each. The procedure works the same, except we add a group_by statement to group by our time period column before we create the rank and P columns. Then, when we plot, we can just tell ggplot to create different colored lines based on the time period names and it will plot a separate flow duration curve for each. Tidyverse FOR THE WIN! Describe the differences in flow regime you see between the three periods of 1960-1980, 1980-2000, and 2000-2020. Qdat &lt;- Qdat %&gt;% mutate(year = year(Date)) %&gt;% mutate(period = case_when( year &lt;= 1980 ~ &quot;1960-1980&quot;, year &gt; 1980 &amp; year &lt;= 2000 ~ &quot;1980-2000&quot;, year &gt; 2000 ~ &quot;2000-2020&quot;)) Qdat &lt;- Qdat %&gt;% group_by(period) %&gt;% mutate(rank = rank(-Flow)) %&gt;% mutate(P = 100 * (rank / (length(Flow) + 1))) Qdat %&gt;% ggplot(aes(x = P, y = Flow, color = period))+ geom_line()+ scale_y_log10()+ xlab(&quot;% Time flow equalled or exceeded&quot;)+ ylab(&quot;Q (cfs)&quot;) 15.7 Compare to a boxplot of the same data We are really just looking at the data distribution here. Remember another good way to compare distributions is a boxplot. Let’s create a boxplot showing flows from these time periods. (we will also mess with the dimensions of the plot so the boxes aren’t so wide using fig.width and fig.height in the ``` header above the code chunk) What are the advantages/disadvantages of the flow duration curves vs. boxplots? Qdat %&gt;% ggplot(aes(x = period, y = Flow)) + geom_boxplot()+ scale_y_log10() 15.8 Challenge: Examining flow regime change at the Grand Canyon The USGS Gage “Colorado River at Yuma, AZ” is below the Hoover dam. The Hoover Dam closed in 1936, changing the flow of the Colorado River below. Using the dataRetrieval package, load average daily discharge data from 10-01-1905 to 10-01-1965 from the Yuma gage. Create a line plot of discharge and flow duration curves to examine the differences in discharge for the periods: 1905 - 1936, 1937 - 1965. Use patchwork to place the figures side-by-side. How does the FDC show the differences you observed in the line plot? siteid &lt;- &quot;09521000&quot; startDate &lt;- &quot;1905-10-01&quot; endDate &lt;- &quot;1965-10-01&quot; parameter &lt;- &quot;00060&quot; 15.9 That’s it! Next we will apply some of these principles to look at low-flow statistics. "],["lab-5---low-flow-analysis.html", "Chapter 16 Lab 5 - Low Flow Analysis", " Chapter 16 Lab 5 - Low Flow Analysis Link here "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
